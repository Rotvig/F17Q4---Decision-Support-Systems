\chapter{Cross Validation}
\label{chp:crossval}
Cross Validation is one of two resampling methods in Statistical Learning, the other being the Bootstrap which will be covered in the next chapter. As the original dataset is a sample, resampling means taking a sample from the dataset and use that in the learning process. This is done by splitting up the data set in different ways, so that the training is done on varying samples. 

\section{The validation set approach}
In the Validation Set Approach the available set of samples are randomly divided into two parts of approximately equal size. One is the training set and the other is the validation set. The idea is then to fit the model on the training set, and then apply the model to the validation set. The resulting error of the validation set is then used as an estimate of the test error. This error is typically found as the Mean Squared Error. 

\myFigure{validationSetApproach.png}{Splitting data randomly}{fig:valid}{1} 

The concept of randomly splitting the dataset is illustrated in Figure \ref{fig:valid}. The dataset is a series of numbers from 1 to n, and the result is here two almost equally sized parts with the numbers from the original dataset randomly distributed. 

The blue half of the dataset is the training set, meaning the part that the model is fit to, and the orange half is the validation set, meaning the part that the model is applied to, in order to predict observations. 

\subsection{Lab 5.3.1}
In this lab exercise the goal is to use the Validation Set Approach. So as described in the section above the first thing to do is to split the data randomly in two halves. The data used here is the \emph{Auto} dataset, containing a list of car informations. 

\begin{lstlisting}[language=Python, label=lst:lst_valid, caption=Auto dataset randomly split]
data = pd.read_csv('Auto.csv', usecols=range(0, 8), na_values='?',
		parse_dates=True).dropna()

df = pd.DataFrame({'mpg': data['mpg'], 'horsepower':  
		pd.to_numeric(data['horsepower'])})

train, test = train_test_split(df, test_size = 0.5)
\end{lstlisting}

Listing \ref{lst:lst_valid} shows the \emph{Auto} data being first read into the program using the pandas library. Since there are some "?" values that are unusable, these are removed by the \emph{.dropna()} function.

Afterwards a dataframe is created containing the \emph{horsepower} and \emph{mpg} information from the dataset. This dataframe is then split randomly in half by the \emph{train\_test\_split()} function. 

\begin{lstlisting}[language=Python, label=lst:fit_valid, caption=Fit linear regression]
X_train = train[['horsepower']]
X_test = test[['horsepower']]

Y_train = train['mpg']
Y_test = test['mpg']

lm = linear_model.LinearRegression()
lm.fit(X_train, Y_train)
\end{lstlisting}

The train and test sets from Listing \ref{lst:lst_valid} are further used in Listing \ref{lst:fit_valid}. Here the \emph{horsepower} and \emph{mpg} data of the splits are further split into X and Y training and test sets. This is to be able to fit a linear model to the training splits, using \emph{X\_train} as the training data and \emph{Y\_train} as the target values.

\begin{lstlisting}[language=Python, label=lst:print_valid, caption=Fit linear regression]
print("Linear Model")
print("Mean squared error: %.2f" 
		% np.mean((lm.predict(X_test) - Y_test) ** 2))
\end{lstlisting}

In Listing \ref{lst:print_valid} the Mean Squared Error is computed by applying the fitted model to the test sets. It is then printed, which is shown in Figure \ref{fig:result_valid}. The error is 24,42. 

To improve upon this it is possible to use polynomials in the linear regression. This is shown in Listing \ref{lst:poly2_valid}.

\begin{lstlisting}[language=Python, label=lst:poly2_valid, caption=Polynomial features with degree = 2]
poly = PolynomialFeatures(degree=2)
X_poly_train_deg2 = poly.fit_transform(X_train)
X_poly_test_deg2 = poly.fit_transform(X_test)

lm.fit(X_poly_train_deg2, Y_train)
\end{lstlisting}

Both second and third degree polynomials are used on the model, but Listing \ref{lst:poly2_valid} only shows the code for the second degree polynomial. However the walkthrough is the same. The training data is fitted to the model, like in Listing \ref{lst:fit_valid}, but the training and test set for \emph{horsepower} has been fitted to a second order polynomial. 

\begin{lstlisting}[language=Python, label=lst:poly_print_valid, caption=Printing polynomial features with degree = 2]
print("Mean squared error: %.2f" 
% np.mean((lm.predict(X_poly_test_deg2) - Y_test) ** 2))
\end{lstlisting}

The Mean Squared Error is printed as in Listing \ref{lst:poly_print_valid}, and the result is shown in Figure \ref{fig:result_valid}.

\myFigure{validationSetApproach_result.png}{Result of Validation Set Approach}{fig:result_valid}{0.4}

The result shows a clear improvement of the Mean Squared Error, but not continuously. Using the second degree polynomial shows a drop in the Mean Squared Error from 24,42 to 19,46, but using the third degree polynomial shows a slight increase from 19,46 to 19,55. This follows the graph in Figure \ref{fig:poly_valid} from the book. 

\myFigure{validationSetApproach_poly.png}{Validation error estimates for a single split}{fig:poly_valid}{0.6}
\section{Leave-one-out cross validation}


\subsection{Lab 5.3.2}
"Taking cross validation to the extreme. Leave one out - we only test on one data point and train on all other.

HIgh computational costs


\section{K-fold cross-validation}


\subsection{Lab 5.3.3}