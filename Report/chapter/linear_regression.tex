\chapter{Linear Regression}
\label{chp:linreg}

Linear regression is an approach in supervised learning. It is used for predicting a quantitative response and it is widely used for statistical learning methods. It may seem that it is a bit simple compared to some of the more modern statistical learning approaches, but it serves as a good base as newer approaches can be seen as an generalization or extensions of the linear regression. 
In this chapter the key ideas in linear regression model and it will describe the least squares approach
which is the most commonly used to fit this model

\section{Lab 3.6.2 - Simple Linear Regression}

Simple linear regression is used for predicting a quantitative response, that is often called Y, based on a single predictor, which is often called X.
The relationship between X and Y can be expressed as:

\begin{equation}
	Y \approx \beta_0 + \beta_1X
\end{equation}

This means that Y is approximately modeled as X or in other word Y regressing on X. 
$\beta_0$ and $\beta_1$ are two unknown constant that represent the intercept and slope in the linear model. They are also known as model coefficients or parameters. 
To be able to predict, the training data is used to produce
an estimation of $\hat{\beta_0}$ and $\hat{\beta_1}$ for the model coefficients. 
The mathematically expression for the computation is:

\begin{equation}
	\hat{\beta_0} = \hat{\beta_0} + \hat{\beta_1}x
\end{equation}


$\hat{y}$ is the prediction of Y based on X = x. The \textit{hat} symbol is used to estimate value for an unknown parameter or coefficient or to predict a value of the response.

\subsection{Estimation the coefficients}

The coefficients 
$\hat{\beta_0}$ and  $\hat{\beta_1}$ are in practice unknown. So in order to use (2.1) to make predictions, $\hat{\beta_0}$ and  $\hat{\beta_1}$ needs to be estimated so the data can be used. 

First Let 
\begin{equation}
(x_1, y_1), (x_2, y_2),..., (x_n, y_n)
\end{equation}

represent n observation pairs where each of them have a measurement of X and a measurement of Y. n can represent lots of thing. can be the number of different markets, or different cities. It depends on the data. 

The goal is to estimate $\hat{\beta_0}$ and  $\hat{\beta_1}$ such that the linear model fits the data as good as possible. In other words $\hat{\beta_0}$ is the intercept and  $\hat{\beta_1}$ is a slope, so the resulting line is as close as possible to n\emph{th} data points

In this lab the \emph{least square} method will be used because it is the most common approach. 

First the prediction for Y will be written as $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i$ based on the \emph{i}th value of X.
Then $e_i = y_i - \hat{y_i}$ will represent the \emph{i}th residual, which is the different between it \emph{i}th observed response value and the \emph{i}th response value that is predicted by the linear model

The \emph{residual sum of squares} also called RSS will be define as

\begin{equation}
RSS = e^2_1 + e^2_2 + ... + e^2_n,
\end{equation}

which can be translated to

\begin{equation}
RSS = (y_1 - \hat{\beta_0} - \hat{\beta_1}X_1)^2 + (y_2 - \hat{\beta_0} - \hat{\beta_1}X_2)^2 + ... + (y_n - \hat{\beta_0} - \hat{\beta_1}X_n)^2
\end{equation}

Now the least squares approach can be used to choose $\hat{\beta_0}$ and $\hat{\beta_1}$ to minimize the RSS (2.5).
The minimization is written as:

\begin{equation}
\dfrac{\hat{\beta_1} = \sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
\end{equation}

\begin{equation}
\hat{\beta_0} = \bar{y} -\hat{\beta_1}\bar{x}
\end{equation}

In Lab 3.6.2 the data set Boston is given. It consists of records about 506 neighborhoods around Boston. The task is to predict \emph{medv} (median house  value) via the predictor \emph{lstat} (percent of households with low socioeconomic status).
To perform the linear regression the library sklearn is used in python. 

\lstset{}
\begin{lstlisting}[caption={Python Linear Regression function}, label=lst:lin_reg, mathescape=true]
lm = linear_model.LinearRegression()
lm.fit(boston_X_train, boston_Y_train)
\end{lstlisting}

In listing \ref{lst:lin_reg} it is shown how a linear regression object is created, and how it is used to fit the data. ‘boston\_X\_train’ is the \emph{lstat} data, and ‘boston\_Y\_train’ is the \emph{medv} data.
The result is plotted in a graph seen in figure \ref{fig:lin_reg_plot}.

\myFigure{lin_reg_plot.PNG}{Linear regression plot of \emph{medv} and \emph{lstat}}{fig:lin_reg_plot}{0.6}

The code in Listing \ref{lst:lin_reg_plot} is used to plot \emph{medv} and \emph{lstat} along with the least squares regression. To plot, the python libary \emph{matplotlib} is used. In line 1 the test data is plotted to see how it corresponds to the the least squares regression. To plot the least squares regression the command 'plot' is used this is seen in line 2. Where 'boston\_X\_test' is the test data which consist of the \emph{lstat} data. 

\begin{lstlisting}[caption={Python Plotting of Linear Regression function}, label=lst:lin_reg_plot, mathescape=true]
plt.scatter(boston_X_test, boston_Y_test,  color='black')
plt.plot(boston_X_test, lm.predict(boston_X_test), color='blue',
linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()
\end{lstlisting}

There are some evidence of non-linearity in the relationship between \emph{lstat} and \emph{medv}. This can be seen in Figure \ref{fig:lin_reg_plot} where a non-linear least squares regression would be a better fit to the data points.


\section{Lab 3.6.3 - Multiple Linear Regression}

The basis for Multiple Linear Regression is the same as for Simple Linear Regression, as it is used to predict a quantitative response. However, where Simple Linear Regression uses a single predictor, Multiple Linear Regression uses two or more predictors. So where there before were a one-to-one relationship between the predictor variable and response variable, there is now a many-to-one relationship, as illustrated in Figure \ref{fig:relation}.

\myFigure{relation.PNG}{Relationship between predictor variables "P.V" and response variables "R.V" in Simple- and Multiple Linear Regression}{fig:relation}{0.8}

As specified in Lab 3.6.2, the predictors are called X, while Y is our response. This means for Multiple Linear Regression, the relationship between X and Y is expressed as:

\begin{equation}
Y \approx \beta_0 + \beta_1 * X_1 + \beta_2 * X_2 + … + \beta_k * X_k
\end{equation}


