\chapter{Conclusion}
\label{chp:conc}

Throughout this report the theory of different subjects within statistical learning was elaborated upon. Related to each topic exercises were implemented in Python to give a deeper understanding for each method. 

Different methods for prediction and how they differ were discussed. Linear regression was the only method presented for predicting a quantitative response variable but it showed to be less appropriate for classification. For classification logistic regression and linear discriminant analysis were explained. Choosing between these two mainly depended on whether the classes had a Gaussian distribution. If this was the case then linear discriminant analysis would likely outperform logistic regression and vice versa. For more non-linear data with more complex decision boundaries quadratic discriminant analysis was presented. If the decision boundary was even more complex then K-nearest neighbor classifier was presented as an option. Therefore no method can be seen as better than the other, since choosing between the prediction methods is all about the type of response and the data.

Resampling methods for getting information about the properties of the models were also explained in the report. Bootstrap can be used for quantifying uncertainty in a model. It showed a good fit for estimating the standard error of coefficients in the exercises. For more complex data the bootstrap method might not be applied as simply.
Validation set approach and cross validation were presented as methods for estimating the test error. Validation set approach was a simpler and less computationally expensive approach but it can lead to and overestimation of the test error. Leave-one-out cross validation and k-fold cross validation were also presented. In general a 5 or 10 fold cross validation is preferred as a good compromise for the bias-variance trade-off.

For subset selection best subset selection along with forward and backward stepwise selection were explained. Best subset selection estimates the most optimal numbers of predictors and which predictors to use more precisely because it iterates through all combinations. However, when more than 40 predictors are available, then best subset selection becomes infeasible. Instead forward and backward stepwise selection can be used as two lot more efficient methods which both often estimates similar models to best subset selection.

When the true label is unknown then unsupervised learning techniques can be used instead. Unlike supervised learning there is no simple goal and the analysis is often more exploratory. K-means clustering and hierarchical clustering was presented in the report. If the data in fact has a hierarchical structure then this is likely to estimate the clusters well. If the data does not have a hierarchical structure then k-means is preferred even though the numbers of clusters must be prespecified. 

The report has given an introduction to all the topics presented in the course \emph{Decision support systems}. Furthermore it has given insight in the advantages and disadvantages of the different methods and understanding when a method is applicable.




