\chapter{Resampling Methods}
\label{chp:crossvalidation}
In this chapter different types of resampling methods are explained. The validation set approach is presented as a simple way of estimating the test error for a model, while cross-validation is presented as a more thorough tool for achieving a better result. Lastly Bootstrap is presented as an equivalent to cross-validation, but with slight alterations. 

Cross-validation in general is done by splitting up the data set in different ways, so that training and testing is done on varying samples. Bootstrap however, clones original  data sets to create multiple  data sets. Also, where cross-validation is a technique to validate the performance of a model, Bootstrap is used to estimate the uncertainty of a certain coefficient. 

\section{The Validation Set Approach}
\label{sec:VSA}
In the validation set approach the available set of samples is randomly divided into two parts of approximately equal size. One is the training set and the other is the validation set. The idea is then to fit the model on the training set, and then apply the model to the validation set. The resulting error of the validation set is then used as an estimate of the test error. This error is typically found as the mean squared error \emph{MSE}. 

\myFigure{validationSetApproach.png}{Splitting data randomly}{fig:valid}{0.6} 

The concept of randomly splitting the  data set is illustrated in figure \ref{fig:valid}. The  data set is a series of observations from 1 to n, and the result is two almost equally sized parts, with the observations from the original  data set randomly distributed. 

The blue half of the  data set is the training set, this is used for fitting the model. The orange half of the data set is the validation set, used for estimating the prediction error rate.


\subsection{Lab 5.3.1 - The Validation Set Approach}
In this exercise the goal is to use the validation set approach. So as described in the section above the first thing to do is to split the data randomly in two halves. The data used here is the \emph{Auto}  data set, containing a list of car information. 

\begin{lstlisting}[language=Python, label=lst:fit_valid, caption=Fitting linear regression]
train, test = train_test_split(df, test_size = 0.5)

X_train = train[['horsepower']]
X_test = test[['horsepower']]

Y_train = train['mpg']
Y_test = test['mpg']

lm = linear_model.LinearRegression()
lm.fit(X_train, Y_train)
\end{lstlisting}

Listing \ref{lst:fit_valid} shows the \emph{Auto} data as \emph{df} being split randomly in half by the \emph{train\_test\_split()} function. 
\emph{horsepower} is used as a predictor and denoted X while \emph{mpg} is the response value denoted Y. The model is then fitted on the training data \emph{X\_train} and \emph{Y\_train}.

\begin{lstlisting}[language=Python, label=lst:print_valid, caption=Printing MSE]
print("Mean squared error: %.2f" 
		% np.mean((lm.predict(X_test) - Y_test) ** 2))
\end{lstlisting}

In Listing \ref{lst:print_valid} the MSE is computed by applying the fitted model to the test sets. The error is 24,42. 

Because the relationship between \emph{horsepower} and \emph{mpg} is non-linear the model can be improved upon by using higher polynomial features for fitting the model. Using second degree polynomials is shown in Listing \ref{lst:poly2_valid}. Third degree polynomials was also applied in the exercise.

\begin{lstlisting}[language=Python, label=lst:poly2_valid, caption=Polynomial features with degree = 2]
poly = PolynomialFeatures(degree=2)
X_poly_train_deg2 = poly.fit_transform(X_train)
X_poly_test_deg2 = poly.fit_transform(X_test)

lm.fit(X_poly_train_deg2, Y_train)
\end{lstlisting}

Using polynomials shows a clear improvement of the MSE, but not continuously. Using the second degree polynomial shows a drop in the MSE from 24,42 to 19,46, but using the third degree polynomial shows a slight increase from 19,46 to 19,55. 

\section{Leave-one-out cross-Validation}
In validation-set approach only half of the data is used for training the model. This might give an overestimate of the mean squared error when training on the complete data set. The MSE might also vary a lot when doing random splits of the  data set. Leave-one-out cross-validation tries to address this drawback and is illustrated in figure \ref{fig:loocv}. It splits the  data set in number of observations n and then uses n-1 observations for training and the last observation as validation set. It then iterates through using all observations as validation. This might address the drawbacks of validation set approach but it is also very time consuming as a model has to be fitted n times.

\myFigure{LOOCV.png}{Leave-one-out cross-validation}{fig:loocv}{0.4}

\subsection{Lab 5.3.2 - Leave-One-Out Cross-Validation}
For the exercise for LOOCV, the \emph{Auto} data is read just like in Lab 5.3.1, but not split into two halves of random values. The \emph{horsepower} and \emph{mpg} data parts are though still distributed into respectively \emph{X\_train} and \emph{Y\_train}. 

\begin{lstlisting}[language=Python, label=lst:LOOCV, caption=Leave-one-out cross-validation loop]
for i in range(1, 6):
	lm = linear_model.LinearRegression()
	poly = PolynomialFeatures(degree=i)
	X_poly_train = poly.fit_transform(X_train)
	
	kf = KFold(n_splits=X_train.shape[0]) 
	test = cross_val_score(lm, X_poly_train, Y_train, cv = kf, 
			scoring = "neg_mean_squared_error")
	
	print(np.mean(-test))
\end{lstlisting}

In Listing \ref{lst:LOOCV} the LOOCV is done. Lines 2-4 are the same functionality as in Listing \ref{lst:poly2_valid}, where polynomials are used for the validation set approach. However, here the degree is iteratively increasing up to a fifth degree polynomial.

The \emph{KFold} function is used for LOOCV to split the data set in as many folds as there are observations, i.e. making 392 folds. Then everything is used as parameters for the \emph{cross\_val\_score} function. It uses the fitted model, the \emph{horsepower} training set fitted to an i'th degree polynomial, the \emph{mpg} training set and the folds to evaluate a score by cross-validation.

The mean squared error is lastly computed from the result of the cross-validation. Once again it is clear that there is improvement when using second degree polynomial, but from there no clear improvement can be seen. Here the MSE is a bit lower than with the validation set approach. This can be caused by the discussed overestimation of the MSE by only training on half the  data set. For first degree polynomial the MSE is 24.23, for second degree polynomial 19.25 and for third degree polynomial the MSE was calculated to 19.33. All of these error rates showed lower than with validation set approach.


\section{K-Fold Cross-Validation}
K-fold cross-validation is similar to LOOCV, however it surpasses LOOCV in some areas. Instead of using only one observation as the validation set, the  data set is split into k roughly equal-sized parts, called \emph{folds}, and one of these folds is the validation set. All other folds are used as training set. Similar to LOOCV the algorithm then iterates through which fold is the validation set. Since there are less iterations to go through the K-fold cross-validation has a computational advantage over the LOOCV, but that is not all. You can also look at what is called the bias-variance trade-off. 

The LOOCV gives an approximately unbiased estimate of test error compared to the validation set approach, due to training on all but one observation. However, in an estimating procedure, another important aspect is the procedure's variance, and this is an aspect where the k-fold cross-validation is superior to LOOCV. When performing LOOCV, the result is an average of \emph{n} fitted models, and since the training set for each model only vary by one observation, the outputs are highly correlated. In k-fold cross-validation the training sets in each model has a smaller overlap, and since the mean of highly correlated outputs has high variance, the LOOCV's results have higher variance than the K-fold cross-validation's results. So the bias-variance trade-off is associated with k in K-fold cross-validation. The higher the k, the more folds, and thereby higher correlation giving higher variance. In practice, $k=5$ or $k=10$ gives a good result \citep[pp. 183]{ISLR}.

\myFigure{K_fold.png}{Representation of K-fold cross-validation}{fig:K_fold}{0.4}

The idea behind k-fold cross-validation is illustrated in Figure \ref{fig:K_fold}, where the blue folds are the training sets and the orange fold is the validation set. Note that the case where k is equal to the number of observations is what was just described as LOOCV. Having a lower value of k therefore provides a less computationally expensive way of doing cross-validation.

\FloatBarrier
\subsection{Lab 5.3.3 - k-Fold Cross-Validation}
The only difference from Lab 5.3.2 to this lab exercise is that now the number of folds is set by k, and not the amount of observations. In Listing \ref{lst:K_fold} it can be seen that the number of splits is set to 10, as the exercise specifies $k=10$, and the loop now runs 10 times, up until a 10th degree polynomial. The rest is similar to Listing \ref{lst:LOOCV}.

\begin{lstlisting}[language=Python, label=lst:K_fold, caption=K-fold cross-validation loop]
for i in range(1, 11):
	...
	k_fold = KFold(n_splits=10, shuffle=True) 
	...
\end{lstlisting}
\FloatBarrier

In table \ref{table:kfold_polynomials_mse} the result of the k-fold cross-validation is shown from a 1 to 10 degree polynomial. The mean squared error is lower than with validation set approach. Compared to LOOCV it has a higher MSE for some polynomials while for others the MSE is lower. As explained this method is computationally less expensive than LOOCV and this was also obvious when executing the implementation.

\begin{table}[htbp]
	\centering
	\begin{subtable}[t]{2in}
		\centering
		\begin{tabular}{ p{2.5cm} p{1.5cm}  }
			\textbf{Degree} & \textbf{MSE} \\
			\hline 
			\\
			1 & 24.22 \\\hline
			\\
			2 & 19.19 \\\hline
			\\
			3 & 19.28 \\\hline
			\\
			4 & 19.54  \\\hline
			\\
			5 & 19.00  \\\hline
		\end{tabular}
		\label{table:mse_validation}
	\end{subtable}
	\quad 
	\begin{subtable}[t]{2in}
		\centering
		\begin{tabular}{ p{2.5cm} p{1.5cm}  }
			\textbf{Degree} & \textbf{MSE} \\
			\hline 
			\\
			6 & 18.85 \\\hline
			\\
			7 & 19.48  \\\hline
			\\
			8 & 19.41  \\\hline
			\\
			9 & 19.02  \\\hline
			\\
			10 & 19.04  \\\hline
		\end{tabular}
		\label{table:mse_cross}
	\end{subtable}
	\caption{Polynomials from 1 to 10 and calculated mean squared error.}\label{table:kfold_polynomials_mse}
\end{table}


\section{Bootstrap}
This chapter is about the bootstrap method which is a statistical tool that can be used to calculate uncertainty associated with estimators or statistical learning methods. It can for example be used to estimate the standard errors of the coefficients from a linear regression fit. Bootstrap is a simple approach which can be adapted and applied to many different statistical learning methods. Bootstrap relies on random sampling with replacement. Replacement means that the same observations can occur more than once or maybe not at all in the bootstrap data set. 

%Link til billede: https://www.draw.io/?lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1#G0Bw37nIXex8aXSl91SjQ4MjlvNXc
\myFigure{bootstrap_example.PNG}{A graphical example of the bootstrap approach}{fig:bootstrap}{0.6}

Figure \ref{fig:bootstrap} is a graphical example of the bootstrap approach on a sample containing 3 observations. These observations are randomly sampled with replacement from the original data set Z. For each bootstrap data set $\hat{\alpha}$ can be calculated.

With all $\alpha$ values estimated the mean and standard deviation can be estimated to reason about the accuracy of the bootstrap data sets.

\subsection{Lab 5.3.4 - The Bootstrap}

The goal with Lab 5.3.4 is to show the use of bootstrap on the example used in Section 5.2 \citep[pp. 187-190]{ISLR} which uses the \emph{Portfolio} data set. The second task of Lab 5.3.4 is to estimate the accuracy of the linear regression model on the \emph{Auto} data set.

\subsubsection{Estimating the Accuracy of a Statistic of Interest}

Initially, a function for calculating $\alpha$ is required. The function is shown in listing \ref{lst:alpha}. The function receives \emph{data} which is a vector with the properties X and Y. The second input is \emph{index} which is used to access the wanted X and Y values in the vector \emph{data}.
With the X and Y values the $\alpha$ can now be calculated and returned.

\begin{lstlisting}[caption={Function for calculating $\alpha$ in python}, label=lst:alpha, mathescape=true]
def alpha(data,index):
	X = data['X'][index]
	Y = data['Y'][index]
	return ((np.var(Y) - np.cov(X,Y)) / (np.var(X) + np.var(Y) - 
		2*np.cov(X,Y)))[0,1]
\end{lstlisting}

The next step is to bootstrap the \emph{Portfolio} data set.
To perform a bootstrap a second function is needed which is seen in listing \ref{lst:bootstrap}.

\begin{lstlisting}[caption={Bootstrap function in python}, label=lst:bootstrap, mathescape=true]
def boot_python(data, function, num_of_iteration):
	n = data.shape[0]
	idx = np.random.randint(0, n, (num_of_iteration, n))
	stat = np.zeros(num_of_iteration)
	for i in xrange(len(idx)):
		stat[i] = function(data, idx[i])
	return {'Mean': np.mean(stat), 'std. error': np.std(stat)}
\end{lstlisting}

The bootstrap method receives three arguments, the first being \emph{data} which is the data that the bootstrap approach should be performed on. The next argument is the function for calculating $\alpha$. The last argument is the number of simulated data that should be created.  

Now everything is set to perform a bootstrap on the \emph{Portfolio} data set with an \emph{num\_of\_iteration} set to 1,000. The result is as follows.

\begin{center}
	$\hat{\alpha} = 0.5834$
\end{center}

\begin{center}
	$SE(\hat{\alpha}) = 0.09102$
\end{center}


\subsubsection{Estimating the Accuracy of a Linear Regression Model}

In the next part of Lab 5.3.4 the goal is to use the bootstrap approach to check the variability for $\beta_0$ and $\beta_1$, which is the intercept and slope terms for the linear regression model. The linear regression model in this exercise uses \emph{horsepower} to make a prediction of \emph{mpg}. \emph{horsepower} and \emph{mpg} are part of the \emph{Auto} data set.

First step is to create a function to calculate the intercept and slope, this is seen in listing \ref{lst:boot}. The function \emph{boot} performs a linear regression fit on X and Y. 
\newpage
\begin{lstlisting}[caption={Boot function in python}, label=lst:boot, mathescape=true]
def boot(data, index):
	X = data['horsepower'][index]
	Y = data['mpg'][index]
	slope, intercept, r_value, p_value, std_err = stats.linregress(X,Y)
	return [intercept, slope]
\end{lstlisting}

The next step is to adjust the bootstrap function from the last part. The modified version is the same function as in Listing \ref{lst:bootstrap}, except the modified function now returns the mean intercept, standard error, mean slope and standard error slope.

Now with the standard errors of 1,000, the bootstrap estimate for the intercept and slope can be computed.
The bootstrap estimates for the standard error intercept and slope are shown below.

\begin{center}
	$SE(\hat{\beta_0}) = 0.8601$
\end{center} 
\begin{center}
	$SE(\hat{\beta_1}) = 0.007335$
\end{center}

The standard errors estimates $SE(\hat{\beta_0})$ and $SE(\hat{\beta_1})$ obtained by performing a linear regression on the \emph{Auto} data set are 0.717 for the intercept and 0.0064 for the slope. These are different from the results which the bootstrap estimates yielded.

The reason is that the \emph{Auto} data set has a non-linear relationship, so using a quadratic model to fit the data should yield a result that are closer to the bootstrap estimates.

So a new function for finding the standard error is needed, and this function can be seen in Listing \ref{lst:bootstrap_q}.

\begin{lstlisting}[caption={Function to calculate standard error with a qudratic model}, label=lst:bootstrap_q, mathescape=true]
def boot_fn2(data, index):
	formula = 'mpg ~ horsepower+I(horsepower**2)'
	model = smf.glm(formula=formula, data=data, subset=index)
	result = model.fit()
	return result.params
\end{lstlisting}

The results it yields with a 1,000 simulated observations:

\begin{center}
	$SE(\hat{\beta_0}) = 2.06$
\end{center}
\begin{center}
	$SE(\hat{\beta_1}) = 0.0328$
\end{center}
\begin{center}
	$SE(\hat{\beta_2}) = 0.00012$
\end{center}

The new values from the quadratic formula shows a closer correspondence between the estimates from the bootstrap and the standard estimates.