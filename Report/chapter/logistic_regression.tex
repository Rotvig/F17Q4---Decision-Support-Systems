\chapter{Logistic Regression}
\label{chp:logreg}
Linear regression discussed in previous chapter assumes a quantitative response value Y. Often classification is needed to solve a problem. An example could be predicting whether a tumor is malignant. With a binary classification problem, linear regression could be used by converting our wanted qualitative response variable into a quantitative response. For example based on the the quantitative tumor size the tumor is malignant or not. If $Y > 0.5$ then a model could classify the tumor as malignant. One problem of using linear regression for this is that our estimate might be outside the interval $[0,1]$, which can not be thought of as probabilities. For example measuring a tumor size of 0 would yield a negative value. Another problem with linear regression is that it can not be extended to a multiple classification problem, it can only be used for binary classification.

Instead logistic regression can be used to avoid the problem with values outside the interval of $[0,1]$. In logistic regression a logistic function is used.

\begin{center}
	$p(X) = \dfrac{1+e^{\beta_0+ \beta_1 X}}{e^{\beta_0 + \beta_1 X}}$ 
\end{center}

By taking the exponential of $\beta_0 + \beta_1 X$ the result will always be equal to or higher than 0. By dividing by the same equation + 1 the prediction will never be higher than 1.

\section{Lab 4.6.1 - The Stock Market Data}
In this lab the data will first be examined before the regression coefficients are estimated. 

With the following python code a matrix with the pairwise correlation among the predictors is printet. Numpy is used for calculating the correlation matrix. Pandas is used creating a table based on the calculated correlation matrix.
\begin{lstlisting}[language=Python, caption=print correlation matrix]
cor = np.corrcoef(preparedData)
labels = ['Year', 'Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume', 
'Today']
df = pd.DataFrame(cor, columns=labels, index=labels)
print df
\end{lstlisting}

The printet correlation matrix is illustrated in figure \ref{fig:lab461}. Here it is shown that there is very little correlation between the different Lag variables and todays return. A interesting correlation can though be seen between year and volume. This indicates that each year the average number of traded stocks increases. 

\myFigure{461.png}{bla}{fig:lab461}{1} 
\FloatBarrier

With the python code in listing asfdas we can plot the volume and in figure \ref{fig:lab461Plot} the result is illustrated. Here the increase in traded stock is obvious.
\begin{lstlisting}[language=Python, caption=print correlation matrix]
plt.plot(preparedData[6], 'ro')
plt.ylabel('Volume')
plt.xlabel('Index')
plt.show()
\end{lstlisting}

\myFigure{461_plot.png}{bla}{fig:lab461Plot}{0.6} 

\section{Lab 4.6.2 - Logistic Regression}
In this lab a logistic regression model is made based on the stock market data. The predictors were examined in Lab 4.6.1 and some of the predictors will be chosen to predict the direction of the stock market. The direction can either be \emph{up} or \emph{down} and hence this is a binary classification problem.

In the exercise the P-values will first be examined. This is done by fitting a logistic model with the predictors \emph{Lag1, Lag2, Lag3, Lag4, Lag5} and \emph{Volume} to predict the \emph{direction}.  The statsmodels python library is used to print a summary of the fitted model. This is shown in listing asdfsaf
\begin{lstlisting}[language=Python, caption=print correlation matrix]
formula = 'Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume'
model = smf.glm(formula=formula, data=data, family=sm.families.Binomial())
print(model.fit().summary())
\end{lstlisting}

\myFigure{Lab462Pvalue.PNG}{bla}{fig:lab462Pvalue}{0.6} 

The printet summary is illustrated in figure \ref{fig:lab462Pvalue}. Here we see that the lowest p-value is \emph{Lag1} but it still has a value of 0.145, which is still quite high and it presents no evidence of any association between \emph{Lag1} and \emph{Direction}. But note here that only using the predictors with the lowest p-value would probably yield the most accurate result when predicting the \emph{Direction}. The reason for this is that some predictors have so little assocation with the \emph{Direction} that they will only make the prediction less accurate. This will also be shown in the following examples.

\FloatBarrier

In Listing asdfsadf we divide our training data in a training set and a test set. We use all data from 2004 and before to train our logistic model and we test in on the data from 2005.  In this implementation all predictors is used to fit the logistic model.

\begin{lstlisting}[language=Python, caption=Logistic regression using all predictors]
X_train = data[:'2004'][['Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume']]
Y_train = data[:'2004']['Direction']

X_test = data['2005':][['Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume']]
Y_test = data['2005':]['Direction']

lr = linear_model.LogisticRegression()
lr.fit(X_train,Y_train)
\end{lstlisting}

Based on our our test data we check how well our logistic model predicts the \emph{Direction}. The implemention is shown in Listing asdfasdf. The model only predicts correctly $48\%$ of the time. With this result changes to the model definately needs to be made.
\begin{lstlisting}[language=Python, caption=Logistic regression using all predictors]
print(lr.score(X_test, Y_test))
print(pd.crosstab(Y_test, lr.predict(X_test),rownames=['True'], colnames=['Predicted'], margins=True))

print(metrics.classification_report(Y_test, lr.predict(X_test)))
\end{lstlisting}

In Listing asdfas we now only use the predictors \emph{Lag1} and \emph{Lag2}. These are the two predictors with the lowest p-value. 

\begin{lstlisting}[language=Python, caption=Logistic regression using only Lag1 and Lag2 as predictors]
X_train = data[:'2004'][['Lag1', 'Lag2']]
Y_train = data[:'2004']['Direction']

X_test = data['2005':][['Lag1', 'Lag2']]
Y_test = data['2005':]['Direction']

lr = linear_model.LogisticRegression()
lr.fit(X_train,Y_train)
\end{lstlisting}

Running the python code in Listing aasdfda for this implementation gave a better result. It predicted correctly $52\%$ of the time. 





