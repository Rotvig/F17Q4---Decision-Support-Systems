\chapter{Discussion}
\label{chp:disc}
On the basis of the different subjects presented in the previous chapters, this chapter will discuss the different aspects of these  topics.

\section{Linear regression}

Firstly linear regression was presented in the report as a simple method for predicting a quantitative response variable. The method assumes a linear relationship between the response and the predictors but can be extended to the use case of non-linearity. This can be done by using polynomial regression which was used in the exercises in chapter \ref{chp:crossvalidation}. The method though have drawbacks, it assumes the form of the function. For example assuming a linear relationship between the predictors and response variable, when the relations is not linear, will result in a very poor fitting of the model. In cases were more flexibility is needed non-parametric approaches such as k-nearest neighbor can be used. Another limitation of linear regression is that it predicts a quantitative response value. As discussed in chapter \ref{chp:logreg} it is possible to modify linear regression to use it for classification but it works poorly.

\section{Classification}

Instead Logistic regression, linear discriminant analysis and K-nearest neighbor classifier was presented as methods for classification. When choosing between logistic regression or linear discriminant analysis an important aspect is that linear discriminant analysis assumes observations to have a Gaussian distribution. If this assumptions is true, then linear discriminant analysis will likely outperform logistic regression and vice versa. So choosing either logistic regression or linear discriminant analysis mainly depends on whether your observations does have a Gaussian distribution or not. 
 
A more flexible but similar classifier to linear discriminant analysis was also presented, the quadratic discriminant analysis. It instead assumes a separate covariance matrix for each class. This can potentially lead to better prediction if this is the case. This also gives a quadratic decision boundary instead of a linear one. FOr even more flexibility the non-parametric approach, k-nearest neighbor classifier, can be used.
\todo{MÃ¥ske lidt mere om KNN.}

\section{Resampling methods}
When using these different predictions techniques presented another important aspect is how to split the data when training and testing. In generally its important not to train and test on the same data because this leads to a underestimate of the error rate. Techniques such as validation set approach and cross validation was presented in the report. Validation set approach was a very simple technique and was not computationally expensive. The validation set approach though splits the data in two equally sized parts which means the fitting of the model is only done on half the data. When the model is fitted and the predictor is run on the test set this approach often overestimates the test error, compared to when training is done on all the data. If a lot of data is available this might be be a big problem. Another method is to use cross-validation which allows to iterate through splits of training and testing data. This makes for a smart way of utilizing the available data, but it is also computationally expensive.

\todo{bootstrap}

\section{Subset selection}
\todo{subset selection}
A difficult task when fitting a model is choosing the best and the right amount of predictors. Best subset selection, forward and backward stepwise selection was presented as techniques for this. Best subset tries all combinations to find the most optimal, but this has the drawback of being very computationally expensive and not feasible if the number of predictors is too high. In the case were the number of predictors is too high, then forward and backward stepwise selection can provide an estimation of the best predictors. When choosing the amount of predictors for a model it is also important to be aware if a smaller number predictors performs approximately equally as one with more predictors, if this is the case then simplicity of the model might be preferred

\section{Unsupervised learning}
The previous predictions methods mentioned are all supervised methods because a true label was known. Another case described in this report is were the true label is unknown. Two different methods were presented in this report for unsupervised learning, namely k-means clustering and hierarchical clustering.
\todo{something about kmeans and hierarchical}