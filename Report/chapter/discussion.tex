\chapter{Discussion}
\label{chp:disc}
On the basis of the different subjects presented in the previous chapters, this chapter will discuss the different aspects of these  topics.

\section{Linear regression}

Firstly linear regression was presented in the report as a simple method for predicting a quantitative response variable. The method assumes a linear relationship between the response and the predictors but can be extended to the use case of non-linearity. This can be done by using polynomial regression which was used in the exercises in chapter \ref{chp:crossvalidation}. The method though have drawbacks, it assumes the form of the function. For example assuming a linear relationship between the predictors and response variable, when the relations is not linear, will result in a very poor fitting of the model. In cases were more flexibility is needed non-parametric approaches such as k-nearest neighbor can be used. Another limitation of linear regression is that it predicts a quantitative response value. As discussed in chapter \ref{chp:logreg} it is possible to modify linear regression to use it for classification but it works poorly.

\section{Classification}

Instead Logistic regression, linear discriminant analysis and K-nearest neighbor classifier was presented as methods for classification. When choosing between logistic regression or linear discriminant analysis an important aspect is that linear discriminant analysis assumes observations to have a Gaussian distribution. If this assumptions is true, then linear discriminant analysis will likely outperform logistic regression and vice versa. So choosing either logistic regression or linear discriminant analysis mainly depends on whether your observations does have a Gaussian distribution or not. 
 
A more flexible but similar classifier to linear discriminant analysis was also presented, the quadratic discriminant analysis. It instead assumes a separate covariance matrix for each class. This can potentially lead to better prediction if this is the case. This also gives a quadratic decision boundary instead of a linear one. For more complex decision boundaries K-nearest neighbor classifier might outperform QDA. K-nearest neighbor classifier is also a non-parametric classifier which makes it very flexible..

\section{Resampling methods}
Two different resampling methods were presented in the report. Both refit a model to samples from the training set to get information about the model. 

Validation-set approach and cross validation is two approaches to obtain information about the test-set prediction error. In general training and testing on the same data is not a good solution because it leads to a underestimate of the error rate. Techniques such as validation set approach and cross validation was presented in the report to overcome this problem. Validation set approach was a very simple technique and was not computationally expensive. The validation set approach splits the data in two equally sized parts. The model is then fit on only half the data which often leads to an overestimation of the test error, compared to when training is done on all the data. If a lot of data is available this problem might not be as big. Another method is to use cross-validation which allows to iterate through splits of training and testing data. This makes for a smart way of utilizing the available data, but it is also computationally expensive.

The boostrap can also be used for estimating properties of a prediction model, this could for example be the standard errors of the coefficients which was done in the exercises. Bootstrapping does this by sampling from simulated data from the true data set and fitting a model each time. Even though bootstrap showed a simple tool in the exercise presented in the report, it can become a lot more difficult with more complex data sets.

\section{Subset selection}
A difficult task when fitting a model is choosing the best and the right amount of predictors. Best subset selection, forward and backward stepwise selection was presented as techniques for this. Best subset tries all combinations to find the most optimal, but this has the drawback of being very computationally expensive and not feasible if the number of predictors is too high. In the case were the number of predictors is too high, then forward and backward stepwise selection can provide an estimation of the best predictors. When choosing the amount of predictors for a model it is also important to be aware if a smaller number predictors performs approximately equally as one with more predictors, if this is the case then simplicity of the model might be preferred

\section{Unsupervised learning}
The previous predictions methods mentioned are all supervised methods because a true label was known. Another case described in this report is were the true label is unknown. Two different methods were presented in this report for unsupervised learning, namely k-means clustering and hierarchical clustering. K-means clustering requires a prespecified  number of clusters which might be difficult. If this is not possible one could consider using hierarchical clustering instead. Note though that hierarchical clustering assumes that the data has a hierarchical structure, if this is not the case k-means is likely to perform better. In general both algorithms will cluster all observations, even outliers are forced into a cluster, which might result in distorted clusters.