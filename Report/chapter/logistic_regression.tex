\chapter{Classification}
Linear regression discussed in previous chapter assumes a quantitative response value Y but often classification is needed to solve a problem. Consider the binary classification problem: Predicting whether a tumor is malignant or not based on the size of the tumor. Linear regression could be used by converting the wanted qualitative response variable into a quantitative response. The quantitative response can be converted into a qualitative by saying that $p(maligant = true | tumorsize) > 0.5$. The model would then classify a tumor as malignant if the probability is higher than 0.5. 

A problem though arises from this, the model will make estimates outside the interval $[0,1]$. Because some predictions are outside of this interval, then the prediction can not be thought of as probabilities. For example measuring a tumor size of 0 would predict a negative value. Another problem with linear regression is that it can not be extended to a classification problem with more than two classes, it can only be used for binary classification. The reason for this is that their might not be a natural ordering between the different classes and the gap between some classes might be bigger than others. This would make it difficult to make an accurate model using a linear regression.

Instead this chapter presents different methods that are more suited for classification.
\section{Logistic Regression}
\label{chp:logreg}
Logistic regression is a classification method. It avoids the problem with values outside the interval of $[0,1]$. In logistic regression a logistic function is used and can be seen in (3.1).

\begin{equation}
	p(X) = \dfrac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
\end{equation}

By taking the exponential of $\beta_0 + \beta_1 X$ the result will always be equal to or higher than 0. By dividing by the same equation + 1 the prediction will never be higher than 1. In the equation $\beta_0$ and $\beta_1$ is similar to what is used in linear regression. $\beta_0$ describes the probability when the predictor is 0 and $\beta_1$ is the regression coefficient that is multiplied  with the predictor. The equation can be extended to multiple logistic regression using multiple predictors. Instead the equation looks as following and are shown in (3.2).

%\begin{center}
%	$p(X) = \dfrac{e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}$ 
%\end{center}

\begin{equation}
	p(X) = \dfrac{e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + ... + \beta_p X_p}}
\end{equation}

%\todo{måske indsæt en figur der viser grafer med linear og logistic}

\section{Lab 4.6.1 - The Stock Market Data}
In this lab the correlation between the predictors and todays return value is examined. Todays return value describes the direction. If the number is negative the stock went down and if its positive the stock went up. The available predictors are 5 lagging indicators \emph{Lag1, Lag2, Lag3, Lag4} and \emph{Lag5} and a \emph{Volume} which describes the number of shares traded in billions.

With the python code shown in Listing \ref{lst:correlation} a matrix with the pairwise correlation among the predictors and todays return is printet. Numpy is used for calculating the correlation matrix. Pandas is used for creating a table based on the calculated correlation matrix.

\begin{lstlisting}[language=Python, label=lst:correlation, caption=Print correlation matrix]
cor = np.corrcoef(preparedData)
labels = ['Year', 'Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume', 
'Today']
df = pd.DataFrame(cor, columns=labels, index=labels)
print df
\end{lstlisting}

The printed correlation matrix is illustrated in figure \ref{fig:lab461}. Here it is shown that there is very little correlation between the different Lag variables and todays return. This also makes sense because the variables are lagging indicators and therefore it only changes after the economy has already changed. For example if \emph{Lag1} describes how the market went up yesterday then its less likely to go up today. An interesting correlation can be seen between \emph{Year} and \emph{Volume}. This indicates that each year the average number of traded stocks increases. 

\myFigure{461.png}{Pairwise correlation matrix between predictors and todays return}{fig:lab461}{1} 
\FloatBarrier

\emph{Volume} is plotted in figure \ref{fig:lab461Plot}. Here the increase in traded stock is obvious.

\myFigure{461_plot.png}{Correlation between Volume and year, higher index describes forward in time}{fig:lab461Plot}{0.45} 

When choosing predictors in our logistic regression model it must be noted that a correlation matrix not necessarily describes the best predictors, at least not if it is multiple predictors. When choosing predictors its important to choose the predictors that actually are associated with the response variable. The following lab will therefore examine the data to find good predictors.

\section{Lab 4.6.2 - Logistic Regression}
\label{sec:lab462}
In this lab a logistic regression model is made based on the stock market data. The predictors were examined in Lab 4.6.1 and some of the predictors will be chosen to predict the direction of the stock market. The direction can either be \emph{up} or \emph{down} and hence this is a binary classification problem.

In the exercise the P-values will first be examined. This is done by fitting a logistic model with the predictors \emph{Lag1, Lag2, Lag3, Lag4, Lag5} and \emph{Volume} to predict the \emph{direction}. The statsmodels python library is used to print a summary of the fitted model. This is shown in listing \ref{lst:summary}
\begin{lstlisting}[language=Python, label=lst:summary, caption=Print summary]
formula = 'Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume'
model = smf.glm(formula=formula, data=data, family=sm.families.Binomial())
print(model.fit().summary())
\end{lstlisting}

\myFigure{Lab462Pvalue.PNG}{Summary of data with associated p-value describing good predictors}{fig:lab462Pvalue}{0.6} 
\FloatBarrier

The printed summary is illustrated in figure \ref{fig:lab462Pvalue}. Here it is  seen that the lowest p-value is \emph{Lag1} but it still has a value of 0.145, which is still quite high and it presents no evidence of any association between \emph{Lag1} and \emph{Direction}. But note here that only using the predictors with the lowest p-value would probably yield the most accurate result when predicting the \emph{Direction}. The reason for this is that some predictors have so little association with the \emph{Direction} that they will only make the prediction less accurate. This will also be shown in the following examples.


In Listing \ref{lst:AllPredictors} the training data is divided into a training set and a test set. Then all the data from 2004 and before is used to train our logistic model and then it is tested on the data from 2005. In this implementation all predictors is used to fit the logistic model.

\begin{lstlisting}[language=Python, label=lst:AllPredictors, caption=Logistic regression using all predictors]
X_train = data[:'2004'][['Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume']]
Y_train = data[:'2004']['Direction']

X_test = data['2005':][['Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume']]
Y_test = data['2005':]['Direction']

lr = linear_model.LogisticRegression()
lr.fit(X_train,Y_train)
\end{lstlisting}

Based on the test data it is examined how well our logistic model predicts the \emph{Direction}. In Listing \ref{lst:LRscore} a score method is called for the logistic regression model. This function returns the mean accuracy of the model. The score shows that the model only predicts correctly $48.8\%$ of the time. Based on this result the model needs to be approved upon.
\begin{lstlisting}[language=Python, label=lst:LRscore, caption=Print mean accuracy]
print(lr.score(X_test, Y_test))
\end{lstlisting}

In Listing \ref{lst:OnlyLag1Lag2} only the predictors \emph{Lag1} and \emph{Lag2} are now used. \emph{Lag1} is included because it has the lowest p-value. \emph{Lag2} is also used as predictor because it has a very low correlation with \emph{lag1} and also the third lowest p-value.
%It has a very low correlation with \emph{Lag1} and also the third lowest p-value.
Note though that the p-value of \emph{Lag2} is only 0.006 higher than the second lowest p-value, which describes \emph{Volume}. \emph{Volume} also has almost twice as high correlation with \emph{Lag1} and this might lead to a better result using \emph{Lag1} and \emph{Lag2} for prediction.

\begin{lstlisting}[language=Python, label=lst:OnlyLag1Lag2, caption=Logistic regression using only Lag1 and Lag2 as predictors]
X_train = data[:'2004'][['Lag1', 'Lag2']]
Y_train = data[:'2004']['Direction']

X_test = data['2005':][['Lag1', 'Lag2']]
Y_test = data['2005':]['Direction']

lr = linear_model.LogisticRegression()
lr.fit(X_train,Y_train)
\end{lstlisting}

Using only \emph{Lag1} and \emph{Lag2} as predictors gave a better result. Running the score function in Listing \ref{lst:LRscore} showed a mean accuracy of $56\%$. By only using \emph{Lag1} and \emph{Lag2} the model has been improved. This combination of predictors also showed the most accurate for predicting the \emph{Direction}. Using p-values for choosing predictors is important and provides information about the association when multiple predictors  are used.

\section{Linear Discriminant Analysis}
\label{chp:lindisana}

In chapter \ref{chp:logreg} it was described how logistic regression could be used for binary classification. It was also stated how it could be extended to  a classification problem with more than two classes. In practice logistic regression is though rarely used for classification problems with more than two classes \citep[pp. 137]{ISLR}. Furthermore a problem with logistic regression is that it does not perform very stable under some circumstance. One big problem with logistic regression is that it might not converge if the classes are well-separated  \citep{convergencefailure}. An example could be the data in the table below. Here we see that the classes are well-separated as $X<0$ then $Y=0$ and if $X>0$ then $Y=1$. Because of this perfect separation the algorithm will never converge because it can keep optimizing.
%\begin{center} 
%	\begin{tabular}{rll}
%		\multicolumn{1}{c}{\textbf{Y}} &
%		\multicolumn{1}{c}{\textbf{X}} \\ \hline
%		0     &  -2  \\[0.05cm] 
%		0     &  -1  \\[0.05cm] 
%		0     &  -5  \\[0.05cm] 
%		1     &    3  \\[0.05cm] 
%		1     &    1  \\[0.05cm] 
%		1     &    6  \\[0.05cm] 
%	\end{tabular}
%\end{center}

\begin{center} 
	\begin{tabular}{l|llllll}
%		\multicolumn{1}{c}{\textbf{Y}} &
%		\multicolumn{1}{c}{\textbf{X}} \\ \hline
		X & -2 & -1 & -5 & 3 & 1 & 6 \\[0.05cm] 
		Y &  0 &  0 &  0 & 1 & 1 & 1  \\[0.05cm] 
	\end{tabular}
\end{center}

Instead a method called Linear Discriminant Analysis(LDA) can be used. In logistic regression we directly model $Pr(Y|X)$. With LDA $P(X|Y)$ is modeled and prediction is done by using Bayes theorem to achieve $P(Y|X)$ and find the most probable class given the predictors. The following variables are used:

\begin{itemize}
	\item Let $\pi_k$ represent the prior probability $P(Y=k)$ for K classes.
	\item Let $f_k(x)$ represent the probability $P(X=x|Y=k)$. So $f_k(x)$ is the probability of the predictors when the class Y is given.
\end{itemize} 

By inserting these in Bayes formula and normalizing to get $P(Y=k|X=x)$ the following can be written.

\begin{equation}
P(Y=k|X = x) = \dfrac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)}
\end{equation}

For the equation the two variables $\pi_k$ and $f_k(x)$ still needs to be estimated. $pi_k$ can be estimated by computing the fraction of the training data that belongs to the k'th class. While $f_k(x)$ can be estimated as a Gaussian. This means that the equation differs when using only 1 predictor to using multiple. When only using one predictor and taking log of the equation it can be rewritten as the following

\begin{equation}
\delta_k(x) = x  \dfrac{\mu_k}{\sigma^2} - \dfrac{\mu_k^2}{2\sigma^2} + log(\pi_k)
\end{equation}

Then the equation is calculated for each class k and is classified for the k that results in the highest probability. The equation for multiple predictors instead uses a $mu$ matrix and a covariance matrix and can be written as following

\begin{equation}
\delta_k(x) = x^T \Sigma^{-1} \mu_k -  \frac{1}{2} \mu_k^{T} \Sigma^{-1} \mu_k + log \pi_k
\end{equation}

Note here that LDA assumes the observation to have a Gaussian distribution. If this is the case then LDA is likely to outperform Logistic regression, while if it is not, then Logistic regression is likely to outperform LDA. 

The means of the Gaussian are simply calculated by summing all values with a specific class and dividing by the total number.
The variance is calculated across all the data with a specific class.

\section{Lab 4.6.3 - Linear Discriminant Analysis}
First a model linear discriminant analysis model is created with the python library sklearn. The model is created based on the two predictors \emph{Lag1} and \emph{Lag2}. 


\begin{lstlisting}[language=Python, label=lst:ldaModel, caption=creating linear discriminant analysis model sklearn]
X_train = data[:'2004'][['Lag1', 'Lag2']]
Y_train = data[:'2004']['Direction']

X_test = data['2005':][['Lag1', 'Lag2']]
Y_test = data['2005':]['Direction']

lda = LinearDiscriminantAnalysis()
fit = lda.fit(X_train,Y_train)
\end{lstlisting}

After creating the model the calculated prior probability, means and coefficients can be read. The prior probability for \emph{Down} is $\pi_1=0.492$ while the prior probability for \emph{Up} is $\pi_2=0.508$. So based on the data there are more days where the stock market went up than down. 

The means $\mu_k$ are described by a matrix. The matrix describes a separate mean for both \emph{Lag1} and \emph{Lag2} and for each predictor a mean for \emph{Up} and \emph{Down}. The mean value for \emph{Lag1} and \emph{Lag2} has a tendency to be positive on days when the stock went up while they are negative when the stock went down.  $\bigl( \begin{smallmatrix} 	 & Lag1 & Lag2	\\ 	 Down & 0.0428 & 0.0339 \\ 	 Up & -0.0395 & -0.0313 \end{smallmatrix} \bigr) $. The coefficients are estimated to $-0.0554$ and $-0.0443$. These are the multipliers for \emph{Lag1} and \emph{Lag2}.

%\todo{Er det vigtigt at vise hvordan vi printer disse værdier?}
%\begin{lstlisting}[language=Python, label=lst:ldaProperties, %caption=printing lda prior mean and coefficients]
%print "Prior probabilities:"
%print lda.priors_
%
%print "Mean:"
%print lda.means_
%
%print "Coefficients:"
%print lda.coef_
%\end{lstlisting}

In listing \ref{lst:ldaScore} the score is printet. The score show that the LDA model predicted correctly in $56\%$ of the predictions on the test data. 
\begin{lstlisting}[language=Python, label=lst:ldaScore, caption=printing lda score]
print(lda.score(X_test, Y_test))
\end{lstlisting}

This shows how linear discriminant analysis performs very similar to logistic regression on the stock market data.

\section{Quadratic discriminant analysis}
Similar to LDA the Quadratic discriminant analysis (QDA) also assumes that the observations are drawn from a Gaussian distribution. The difference lies in the fact that LDA assumes a shared mean and covariance for the observations for all classes K, while QDA assumes a different mean and covariance for the observations for each class.

\myFigure{lda_vs_qda.PNG}{LDA vs QDA}{fig:qdaCompare}{0.6}

In Figure \ref{fig:qdaCompare} the difference between LDA and QDA is illustrated. It shows the linear decision boundary of LDA, while the QDA can learn quadratic boundaries.

\subsection{Lab 4.6.4}
Again the stock market data is used for prediction with \emph{Lag1} and \emph{Lag2} as predictors. The QDA model is estimated with sklearn. This is shown in listing \ref{lst:qdaModel}.

\begin{lstlisting}[language=Python, label=lst:qdaModel, caption=Creating quadratic discriminant analysis model sklearn]
X_train = data[:'2004'][['Lag1', 'Lag2']]
Y_train = data[:'2004']['Direction']

X_test = data['2005':][['Lag1', 'Lag2']]
Y_test = data['2005':]['Direction']

qda = QuadraticDiscriminantAnalysis()
fit = qda.fit(X_train,Y_train)
\end{lstlisting}

In listing \ref{lst:qdaScore} the score for the QDA model is printed. The print shows that on the test data the prediction was correct in $59.9\%$ of the cases. With the stock market data it actually performed better than both logistic regression and linear discriminant analysis. 
\begin{lstlisting}[language=Python, label=lst:qdaScore, caption=Printing qda score]
print(qda.score(X_test, Y_test))
\end{lstlisting}

\section{K-Nearest Neighbors}
K-Nearest Neighbors or \emph{KNN} is a classification technique that classifies observations depending on distance to k other observations. 

Say you have a dataset with two classes of observations A and B, with 10 observations each. Then a test observation is desired to be classified with $k=5$, which means the five nearest observations will be examined. If three of the examined observations are A and the other two are B then the estimated probabilities are $3/5$ for class A and $2/5$ for class B. 

\subsection{Lab 4.6.5}

In this lab exercise the method \emph{K-nearest Neighbors} is used. To perform this method, sklearn has a function called \emph{KNeighborsClassifier}. This takes one parameter, which is number of n neighbors. After this the function fit is called, that fits the data.
The code is shown in listing

\begin{lstlisting}[caption={Python K-Nearest neighbors function, where K is set to 1}, label=lst:kneighbor, mathescape=true]
X_train = data[:'2004'][['Lag1', 'Lag2']]
Y_train = data[:'2004']['Direction']

X_test = data['2005':][['Lag1', 'Lag2']]
Y_test = data['2005':]['Direction']


knn = KNeighborsClassifier(n_neighbors=1)
fit = knn.fit(X_train, Y_train)
\end{lstlisting}

The function is used to predict the market's movement from the dates in 2005. If the value K is set to 1, 50\% of the predictions are correctly predicted which is not very good.
If K is change to 3, the predictions turns out to be correct in 53\% of the cases. So increasing the number of k from 1 to 3 made the model a bit better. All kinds of different values can be chosen for K, but it is important not to change the number to often because the model then will be customized to the training data which is not good.

If the results are compared with the results that \emph{QDA} produces, the \emph{QDA} provides better results than K-nearest neighbors.