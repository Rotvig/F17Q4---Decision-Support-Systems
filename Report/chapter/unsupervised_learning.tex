\chapter{Unsupervised Learning}
\label{chp:unsuplea}
In unsupervised learning there is no response variable Y. Ther is only a set of features $X_1, X_2,..., X_p$ measured on n observations. So the goal is to find interesting things about the measurements on the features.
Compared to supervised learning unsupervised learning comes with some challenges.the tasks tends to be more subjective and there is no simple goal such as prediction of a response as there is in supervised learning. Furthermore there is no universally accepted mechanism for performing cross validation and how to validate the results on a data set due to there is no Y to check results up against with.
But it is an important tool to predict subgroups when the number of them are unknown.
In this section the focus will be on the  technique called \emph{clustering} which is a used to discover unknown \emph{subgroups} or \emph{clusters}in data set. This is done to divide the data set into distinct groups so that the observations in the data within each of the groups are similar to each other and the observations belonging to other groups are different compared to other groups. 
The problem is to determine if observations are similar or different to each other.
 
\section{K-means clustering}
\label{chp:clus}
K-means clustering is a simple technique to partition the observation in a data set into a pre-specified \emph{k} number of clusters. When \emph{k} is spcified the K-means algorithm wil assign each of the observations to one of the \emph{k} clusters.
Figure \ref{fig:K-meanex} shows a result of a K-mean algorithm where \emph{k} is set to 3. The data set contains of 50 observation which is divide into three cluster. each cluster is marked in a color. The cross is the center points of a cluster. This point is also called \emph{centroids}.

\myFigure{K-mean.PNG}{Example on the K-mean algorithm where K is 3}{fig:K-meanex}{0.8}

K-Means Clustering needs to satisfy two properties. namely
\begin{itemize}
	\item Each observation need to belongs to at least one of the \emph{k} clusters
	\item Cluster cannot overlap, meaning that no observation can belong to more than one cluster.
\end{itemize} 

To get a good clustering the \emph{within-cluster variation} needs to be as small as possible. In other words the algorithm will partition the observation into K clusters, such that the total sum of all within-cluster variation is as small as possible. The within-cluster variation is common calculated with \emph{squared Euclidean distance}.
So the within-cluster variation for the \emph{k}th cluster is the sum of all pairwise squared Eclidean distances between each of the observation in the \emph{k}th cluster divided by the number of observations in the \emph{k}th cluster.
So the algorithm tries to minimize the total within-cluster variation, summed over all \emph{k}th cluster.
The K-means Clustering algorithm can then be written as.

\begin{algorithm}
	\caption{K-Means Clustering}
	\label{algo:KMeansClustering}
	\begin{algorithmic}[1]
		\State Randomly assign a number, from 1 to K, to each of the observations. These servce as initial cluster assignments for the observations. 
		\State Iterate until the cluster assignments stop changing:
		\begin{enumerate}[label=(\alph*)]
			\item For each of the K cluster, compute the cluster \emph{centroid}. The \emph{k}th cluster centroids is the vector of the p feature means for the observations in the \emph{k}th cluster.
			\item Assign each observation to the cluster whose centroid is closet (where closest is defined using Euclidean distance) 
		\end{enumerate}
	\end{algorithmic}
\end{algorithm}

 In step 2(a) the means for each feature are those who minimize the sum of squared deviations. In step 2(b) the reallocating means, that the observations can only improve.
 So when the algorithm run the clustering will continually improve until the result no longer changes. This is called a \emph{local optimum}.
\subsection{Lab 10.5.1}
First a random data set consisting of 50 observations is generated. After this, the \emph{KMeans} from the sklearn library is used to perform the K-mean clustering.
After the algorithm is used, the labels and the centroids are saved. The code is showed in listing \ref{lst:kmeanscode} below with the value of 2 for k
\begin{lstlisting}[language=Python, label=lst:kmeanscode, caption=The KMean function and the code who saved the labels and centroids]
k = 2
kmeans = KMeans(n_clusters=k, init="k-means++",n_init= 20).fit(zip(X,Y))
Labels = kmeans.labels_
centroids = kmeans.cluster_centers_
\end{lstlisting}

The Kmeans also takes two other parameters. namely the \emph{init} and the \emph{n\_init}. Init states which approach that is used, and in this case the \emph{k-means++} approach are used. The n\_init is how many time the approach should be performed. Here it is set to 20.
Labels contains all of the observations and centroids contains the center of the clusters.

Now the task is to plot the labels and the centroids so the two clusters can be visualized. 
This is done with this code shown in listing \ref{lst:kmeansplot}

\begin{lstlisting}[language=Python, label=lst:kmeansplot, caption=The code for plotting the two clusters]
lines = plt.plot(centroids[:,0], centroids[:,1], 'kx')
plt.setp(lines, ms=15.0)
plt.setp(lines, mew=3.0)

for i in range(k):
dsx = X[np.where(Labels==i)]
dsy = Y[np.where(Labels==i)]
plt.plot(dsx, dsy, 'o')

plt.show()
\end{lstlisting}

First the two centroids are plotted. this is done from line 1 to 3. The \emph{'kx'} is written so they are labeled with a cross.

For plotting the observations a for loop is used. i runs through the two possible clusters. then i takes the X and Y values for the observations assigned to the current cluster. This happens from line 5 - 7 and from line 8-10 the observations are plotted and shown. the \emph{'o'} is added so they are plotted as a dot. 
The result of this code can be seen in figure \ref{fig:kmeansResult}

\myFigure{KmeansResult.PNG}{The plotted result of the lab 10.5.1}{fig:kmeansResult}{0.7}

The centroids are marked with crosses and each observation a either a blue or a green dot depending on which cluster it is assign to.


\section{Hierarchical clustering}
K-means clustering has one potential disadvantage namely that it require a pre-specify number of clusters \emph{k}. Here Hierarchical clustering can provide an alternative approach which does not require a pre-specified \emph{k}.
Furthermore Hierarchical clustering results in a tree based representation of the observation in the data set called a \emph{dendrogram}
This section will have focus on the \emph{bottom-up} approach also called \emph{agglomerative} clustering and it is the most common type of hierarchical clustering It is called the \emph{bottom-up} because the dendrogram is built starting from the leaves and combing clusters up to the trunk. 
First the dendrogram and how to build them will be describe then the hierarchical clustering algorithm will be shown.

\subsection{Dendrogram}
The easiest way of explaining a dendrogram is to show one. So figure 10.1 is a dengrogram which shows a data set with 50 observations in two-dimensional space.
The true class labels for each observation are shown in distinct colors. But if these true class labels where not available and the data was observed without them then all will be labeled with the same distinct color. 
One of the leaf represents one of the 50 observations. 
Depending on the data set and the observations the dendrogram can help to determine the number of clusters k that fits this data best. In this example, the number of cluster are two. Line no. one shows the cut in the dendrogram to get the no of cluster to be two. Line no. two shows a different cut, and if this was best for the data set the number of clusters whould be three.
Observations that fuse close to the top of the tree tends to be quite different where observations in the bottom tends to be quite similar. 

\myFigure{Dendrogram.PNG}{Dendrogram with 50 observations}{fig:dendrogram}{1}

To show how this is build a simple data set containing 7 observations are shown in figure 8.3 to the right.
Then it can be seen that observations 5 and 7 are quite similar to each other and observations 1 and 6 are also quite similar to each other. These are the bottom of the dendrogram. Now the dendrogram is expand by looking at the next observation that are close to 5 and 7. It can be seen that 3 are close to them, so this is the next leaf. The same is done for observation 1 and 6. At the end the dendrogram will look like fingure on the left in figure 8.3 

\myFigure{DendrogramData.PNG}{Illustration of a dendrogram and its data set}{fig:dendrogramdata}{1}

\subsection{The Hierarchical Clustering Algorithm}
The dendrogram is obtained via a simple algorithm. The first thing it does is to defining some sort of dissimilarities measture between each pair. This is often Euclidean distance that is used. Then the algorithm proceeds iteratively. It starts at the bottom of the dendrogram and each of the n observations is treaded as its own cluster. Then it finds the two cluster that are closed and fused those two so that there now are n - 1 clusters. Then the next two cluster that are most similar is fused so now there is n - 2 clusters. This is proceeded until all of the observations belongs to one single cluster and the dendrogram is complete.

The Hierarchical Clustering algorithm is shown below.

\begin{algorithm}
	\caption{Hierarchical Clustering}
	\label{algo:HierarchicalClustering}
	\begin{algorithmic}[1]
 		\State Begin with n observations and a measure (such as Euclidean distance) of all the $(\dfrac{n}{2}) =n(n-1)/2 $ pairwise dissimilarities. Treat each observation as its own cluster 
 		\State for \emph{i} $= n,n-1,...,2:$
 		\begin{enumerate}[label=(\alph*)]
 			\item Examine all pairwise inter-cluster dissimilarities among the \emph{i} clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed 
 			\item Compute the new pairwise inter-clusterdissimilarities among the $\emph{i}-1$ remaining clusters  
 		\end{enumerate}
 	\end{algorithmic}
 \end{algorithm}

But one issue has bot been addressed. How does the algorithm determine that cluster 5 and 7 should be fused with cluster 3? The algorithm needs to be cable of define the dissimilarities between two cluster if one or both contains multiple observations. This is done with a extension by using \emph{linkage} which defines dissimilarities between two groups of observations. The most common types of linkage is \emph{complete}, \emph{average}, \emph{single} and
\emph{centroid}.

These are explained in table XX
\begin{center}
	\begin{tabular}{ | l | p{12cm} |}
		\hline
		Linkage & Description \\ \hline
		Complete & Maximal intercluster dissimilarity. Record the largest of these dissimilarities in cluster A and B \\ \hline
		Single & Minimal intercluster dissimilarity. Record the smallest of these dissimilarities in cluster A and B \\ \hline
		Average & Mean intercluster dissimilarity. record the average of pairwise dissimilarities in cluster A and B\\
		\hline
		Centroid & Dissimiliarity between the centroid for cluser A and B and record the average of these dissimilarities
		\\
		\hline
	\end{tabular}
\end{center}

Complete and average linakge are generally the most preferred linakge as they produce more balanced dendrograms. 

Sometimes you need to scale your data set to obtain standard deviation. Then there arise new problems, because which dissimilarity measure should be used and which linkage should be used. This is a difficult problem and there is no standard answer.

\subsection{Lab 10.5.2}
A data set containing 50 observations are randomly generated and assigned in X and Y coordinates. So it is the same data set structure from lab 10.5.1.
The hierarchical clustering dendrogram will be produced using complete, single and average linkage clustering with Euclidean distance.
To perform the hierarchical clustering the library scipy in imported. This provide the functions for creating dendrograms with the linkage that is needed. 
The code for creating the three types of linkage are shown in listing
\ref{lst:Linkage}

\begin{lstlisting}[language=Python, label=lst:Linkage, caption=The code that apllies the three different linkage on the data set]
complete = linkage(zip(X,Y), method='complete', metric='euclidean')
single = linkage(zip(X,Y), method='single', metric='euclidean') 
average = linkage(zip(X,Y), method='average', metric='euclidean')
\end{lstlisting}

As it can been seen, the linkage functions look a lot like each other. The only difference is the method. The two other parameters are the data set and specifies that the linkage should use Euclidean distance. 
Now is it time for creating the dendrograms. To create them the function \emph{dendrogram} from the scipy library is used. The only parameter that is needed to build them is the three linkage objects that was made in listing \ref{lst:Linkage}
The code that plots the three dendrograms are listed in listing \ref{lst:plottingDendrogram}

\begin{lstlisting}[language=Python, label=lst:plottingDendrogram, caption=The python code that plots the dendrograms for the three different linakge]
plt.figure(figsize=(25, 10))
plt.title('Dendrogram for Compplete')
plt.xlabel('sample index')
plt.ylabel('distance')
dendrogram(complete)

plt.figure(figsize=(25, 10))
plt.title('Dendrogram for single')
plt.xlabel('sample index')
plt.ylabel('distance')
dendrogram(single)

plt.figure(figsize=(25, 10))
plt.title('Dendrogram for average')
plt.xlabel('sample index')
plt.ylabel('distance')
dendrogram(average)
\end{lstlisting}

The construction of the diagram is the same for the three different linkage. First the figure size is set, then the title of the dendrogram and in line 3 and 4 the labels for the X-axis and the Y-axis is set. 
The only thing that differ these diagrams is which linkage object that are send as a parameter to the dendrogram function.
The result of each of the three different dendrogram can be seen in figure- \ref{fig:dendrogramcomplete}, \ref{fig:dendrogramsingle} and \ref{fig:dendrogramaverage}

\myFigure{DendrogramComplete.PNG}{The dendrogram for the linkage method complete}{fig:dendrogramcomplete}{0.}

\myFigure{DendrogramSingle.PNG}{The dendrogram for the linkage method single}{fig:dendrogramsingle}{0.5}

\myFigure{DendrogramAverage.PNG}{The dendrogram for the linkage method average}{fig:dendrogramaverage}{0.5}
