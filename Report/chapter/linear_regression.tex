\chapter{Linear Regression}
\label{chp:linreg}

%Linear regression is an approach in supervised learning. It is used for predicting a quantitative response and it is widely used for statistical learning methods. It may seem that it is a bit simple compared to some of the more modern statistical learning approaches, but it serves as a good base as newer approaches can be seen as a generalization or extension of the linear regression. 
%This chapter presents the key ideas of a linear regression model and it will describe the least squares approach
%which is the most commonly used to fit this model\\

This chapter presents Linear regression which is an approach in supervised learning that can be used for predicting a quantitative response. Furthermore the least squares approach is explained as the most commonly used method for fitting a linear regression model.

\section{Simple Linear Regression}

Simple linear regression is used for predicting a quantitative response, often denoted Y, based on a single predictor, often denoted X.
The relationship between X and Y can be expressed as:

\begin{equation}
Y \approx \beta_0 + \beta_1X
\end{equation}

In equation (2.1) $\beta_0$ represents the intercept and $\beta_1$ which is multiplied with X represents the slope.
%This means that Y is approximately modeled as X or in other words Y is regressing on X. 
%$\beta_0$ and $\beta_1$ are two unknown constants that represent the intercept and slope in the linear model. They are also known as model coefficients or parameters. 
In linear regression training data containing observations of X and Y is used to estimate $\hat{\beta_0}$ and $\hat{\beta_1}$. 
This can be mathematically expressed as:

\begin{equation}
\hat{y} = \hat{\beta_0} + \hat{\beta_1}x
\end{equation}

%$\hat{y}$ is the prediction of Y based on X = x. The \textit{hat} %symbol is used to denote the estimated value for an unknown parameter or coefficient or to predict a value of the response.

%When estimating the coefficients 
%$\hat{\beta_0}$ and  $\hat{\beta_1}$ are in practice unknown. So %in order to use (2.1) to make predictions, $\hat{\beta_0}$ and  %$\hat{\beta_1}$ needs to be estimated so the data can be used. 

%First Let 
%\begin{equation}
%(x_1, y_1), (x_2, y_2),..., (x_n, y_n)
%\end{equation}

%represent n observation pairs where each of them have a measurement of X and a measurement of Y. n can represent lots of thing. can be the number of different markets, or different cities. It depends on the data.

$\hat{\beta_0}$ and  $\hat{\beta_1}$ must be estimated to fit all the observations as good as possible. This can be ensured by using the least squared approach.
%In other words $\hat{\beta_0}$ is the intercept and  $\hat{\beta_1}$ is a slope, so the resulting line is as close as possible to n\{th} data points

%First the prediction for Y will be written as $\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i$ based on the \emph{i}th value of X.
%Then $e_i = y_i - \hat{y_i}$ will represent the \emph{i}th residual, which is the difference between the \emph{i}th observed response value and the \emph{i}th response value that is predicted by the linear model.

Least squared approach determines the $\hat{\beta_0}$ and  $\hat{\beta_1}$ to minimize the residual sum of squares.
The residual is defined as the difference between the predicted and the true response value: $y-\hat{y}$. $\hat{y}$ can be replaced with $\hat{\beta_0} + \hat{\beta_1}x$ and the residual sum of squares can then be defined as:

\begin{equation}
RSS = (y_1 - (\hat{\beta_0} + \hat{\beta_1}X_1))^2 + (y_2 - (\hat{\beta_0} + \hat{\beta_1}X_2))^2 + ... + (y_n - (\hat{\beta_0} + \hat{\beta_1}X_n))^2
\end{equation}

%The least squares approach can then be used to choose $\hat{\beta_0}$ and $\hat{\beta_1}$ to minimize the RSS (2.5).
%The minimization is written as:

The least squares coefficients can then be estimated from the minimizers of RSS defined in equation in (2.4) and (2.5).
\begin{equation}
\hat{\beta_1} = \dfrac{\sum\limits_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}
\end{equation}

\begin{equation}
\hat{\beta_0} = \bar{y} -\hat{\beta_1}\bar{x}
\end{equation}

%n (3.4) $\hat{y} \equiv \dfrac{1}{n}\sum\limits_{i=1}^ny_i$ and %$\hat{x}\equiv \dfrac{1}{n}\sum\limits_{i=1}^nx_i$ are the ample eans and the function is used to define the least squares coefficient estimates for a simple linear regression 


\subsection{Lab 3.6.2 - Simple Linear Regression}

In Lab 3.6.2 the data set \emph{Boston} is given. It consists of records from about 506 neighborhoods around Boston. The task is to predict \emph{medv} (median house value) via the predictor \emph{lstat} (percent of households with low socioeconomic status).

The data is initially prepared before the linear regression can be performed. This is shown in listing \ref{lst:lin_reg_prep}. The data is loaded and then the \emph{lstat} data is added to \emph{boston\_X\_train} and the \emph{medv} data is added to \emph{boston\_Y\_train}.

\begin{lstlisting}[caption={Data preparation for linear regression}, label=lst:lin_reg_prep, mathescape=true]
boston = datasets.load_boston()

#Extract 'lstat'-
boston_X_train = boston.data[:, np.newaxis, 12]

#Extract 'medv'
boston_Y_train = boston.target
\end{lstlisting}

To perform the linear regression the library sklearn is used in python. In listing \ref{lst:lin_reg} it is shown how a linear regression model is fitted to the training data \emph{boston\_X\_train} and the \emph{boston\_Y\_train}.

\begin{lstlisting}[caption={Python linear regression function}, label=lst:lin_reg, mathescape=true]
lm = linear_model.LinearRegression()
lm.fit(boston_X_train, boston_Y_train)
\end{lstlisting}

\FloatBarrier

The code in listing \ref{lst:lin_reg_plot} is used to plot \emph{medv} and \emph{lstat} along with the least squares regression line. In line 1 the test data is plotted to see how it corresponds to the least squares regression line. To plot the least squares regression line the command 'plot' is used. This is seen in line 2 where 'boston\_X\_test' is the test data which consist of the \emph{lstat} data. The result is plotted in Figure \ref{fig:lin_reg_plot}. The plots shows that the relation between \emph{lstat} and \emph{medv} is somewhat non-linear. 

\begin{lstlisting}[caption={Python plotting of linear regression function}, label=lst:lin_reg_plot, mathescape=true]
plt.scatter(boston_X_test, boston_Y_test,  color='black')
plt.plot(boston_X_test, lm.predict(boston_X_test), color='blue',
linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()
\end{lstlisting}

\myFigure{lin_reg_plot.PNG}{Linear regression of \emph{medv} and \emph{lstat}}{fig:lin_reg_plot}{0.5}

\section{Multiple Linear Regression}

When using simple linear regression only one predictor is used. But often more than one predictor is available. Instead multiple linear regression uses two or more predictors to predict the response variable Y. The relationship between the predictor variable and response variable with simple linear regression can be described as a one-to-one, while multiple linear regression is a many-to-one relationship, as illustrated in Figure \ref{fig:relation}.

\myFigure{relation.PNG}{Relationship between predictor variables "P.V" and response variables "R.V" in Simple- and Multiple Linear Regression}{fig:relation}{0.8}

\FloatBarrier
As specified in Lab 3.6.2, the predictors are called X, while Y is the response. This means for multiple linear regression, that the relationship between X and Y is expressed as:

\begin{equation}
Y \approx \beta_0 + \beta_1 * X_1 + \beta_2 * X_2 + â€¦ + \beta_k * X_k
\end{equation}

Similar to simple linear regression, the regression coefficients are estimated using the least squares approach so that $\beta_0$, $\beta_1$,...$\beta_p$ is chosen to minimize the RSS.

\subsection{Lab 3.6.3 - Multiple Linear Regression}

In this exercise multiple linear regression is used. In Listing \ref{lst:mul_lin_reg} all 13 predictors in the Boston data set is used for performing multiple linear regression.

\begin{lstlisting}[caption={Linear regression of multiple predictors}, label=lst:mul_lin_reg, mathescape=true]
boston_X_train = boston.data
boston_Y_train = boston.target

lm = linear_model.LinearRegression()
lm.fit(boston_X_train, boston_Y_train)
\end{lstlisting}

%In listing \ref{lst:print} the calculated coefficients, intercept and variance for the linear regression model is printed.

%\begin{lstlisting}[caption={Printing regression overview}, label=lst:print, mathescape=true]
%print("Coefficients:")
%print(lm.coef_)
%print("Intercept: ")
%print(lm.intercept_)
%print("Variance score: %.2f" % lm.score(boston_X_test, boston_Y_test))
%\end{lstlisting}

The printed output is shown in figure \ref{fig:multiOutput}. The output first shows the estimated coefficients for the linear regression model. The first value -0.1072 is $\beta_1$ and is multiplied with the predictor \emph{crim}. The next value 0.0464 is $\beta_2$ and is multiplied with the predictors \emph{zn}. In total there are 13 coefficients, one for each predictor.

Furthermore the intercept value $\beta_0$ is printed. This is the value of the interception with the y-axis. Lastly multiple $R^2$ is printed. The $R^2$ of the simple linear regression was only calculated to 0.544, while for the multiple linear regression model including all 13 predictors the $R^2$ coefficient was improved to 0.74. This indicates that the model is improved using multiple linear regression.

\myFigure{MultiLinRegOutput.PNG}{Resulting coefficients, intercept value and multiple $R^2$ for the multiple linear regression model}{fig:multiOutput}{0.8}


