\chapter{Cross-validation}
\label{chp:crossval}
Cross-validation is one of two resampling methods in Statistical Learning, the other being the Bootstrap which will be covered in the next chapter. As the original dataset is a sample, resampling means taking a sample from the dataset and use that in the learning process. This is done by splitting up the data set in different ways, so that the training is done on varying samples. 


\section{The validation set approach}
\label{sec:VSA}
In the Validation Set Approach the available set of samples are randomly divided into two parts of approximately equal size. One is the training set and the other is the validation set. The idea is then to fit the model on the training set, and then apply the model to the validation set. The resulting error of the validation set is then used as an estimate of the test error. This error is typically found as the Mean Squared Error. 

\myFigure{validationSetApproach.png}{Splitting data randomly}{fig:valid}{0.6} 

The concept of randomly splitting the dataset is illustrated in Figure \ref{fig:valid}. The dataset is a series of numbers from 1 to n, and the result is here two almost equally sized parts with the numbers from the original dataset randomly distributed. 

The blue half of the dataset is the training set, meaning the part that the model is fit to, and the orange half is the validation set, meaning the part that the model is applied to, in order to predict observations. 

\subsection{Lab 5.3.1}
In this lab exercise the goal is to use the Validation Set Approach. So as described in the section above the first thing to do is to split the data randomly in two halves. The data used here is the \emph{Auto} dataset, containing a list of car informations. 

\begin{lstlisting}[language=Python, label=lst:lst_valid, caption=Auto dataset randomly split]
data = pd.read_csv('Auto.csv', usecols=range(0, 8), na_values='?',
		parse_dates=True).dropna()

df = pd.DataFrame({'mpg': data['mpg'], 'horsepower':  
		pd.to_numeric(data['horsepower'])})

train, test = train_test_split(df, test_size = 0.5)
\end{lstlisting}

Listing \ref{lst:lst_valid} shows the \emph{Auto} data being first read into the program using the pandas library. Since there are some "?" values that are unusable, these are removed by the \emph{.dropna()} function.

Afterwards a dataframe is created containing the \emph{horsepower} and \emph{mpg} information from the dataset. This dataframe is then split randomly in half by the \emph{train\_test\_split()} function. 

\begin{lstlisting}[language=Python, label=lst:fit_valid, caption=Fit linear regression]
X_train = train[['horsepower']]
X_test = test[['horsepower']]

Y_train = train['mpg']
Y_test = test['mpg']

lm = linear_model.LinearRegression()
lm.fit(X_train, Y_train)
\end{lstlisting}

The train and test sets from Listing \ref{lst:lst_valid} are further used in Listing \ref{lst:fit_valid}. Here the \emph{horsepower} and \emph{mpg} data of the splits are further split into X and Y training and test sets. This is to be able to fit a linear model to the training splits, using \emph{X\_train} as the training data and \emph{Y\_train} as the target values.

\begin{lstlisting}[language=Python, label=lst:print_valid, caption=Fit linear regression]
print("Linear Model")
print("Mean squared error: %.2f" 
		% np.mean((lm.predict(X_test) - Y_test) ** 2))
\end{lstlisting}

In Listing \ref{lst:print_valid} the Mean Squared Error is computed by applying the fitted model to the test sets. It is then printed, which is shown in Figure \ref{fig:result_valid}. The error is 24,42. 

To improve upon this it is possible to use polynomials in the linear regression. This is shown in Listing \ref{lst:poly2_valid}.

\begin{lstlisting}[language=Python, label=lst:poly2_valid, caption=Polynomial features with degree = 2]
poly = PolynomialFeatures(degree=2)
X_poly_train_deg2 = poly.fit_transform(X_train)
X_poly_test_deg2 = poly.fit_transform(X_test)

lm.fit(X_poly_train_deg2, Y_train)
\end{lstlisting}

Both second and third degree polynomials are used on the model, but Listing \ref{lst:poly2_valid} only shows the code for the second degree polynomial. However the walkthrough is the same. The training data is fitted to the model, like in Listing \ref{lst:fit_valid}, but the training and test set for \emph{horsepower} has been fitted to a second order polynomial. 

\begin{lstlisting}[language=Python, label=lst:poly_print_valid, caption=Printing polynomial features with degree = 2]
print("Mean squared error: %.2f" 
% np.mean((lm.predict(X_poly_test_deg2) - Y_test) ** 2))
\end{lstlisting}

The Mean Squared Error is printed as in Listing \ref{lst:poly_print_valid}, and the result is shown in Figure \ref{fig:result_valid}.

\myFigure{validationSetApproach_result.png}{Result of Validation Set Approach}{fig:result_valid}{0.4}

The result shows a clear improvement of the Mean Squared Error, but not continuously. Using the second degree polynomial shows a drop in the Mean Squared Error from 24,42 to 19,46, but using the third degree polynomial shows a slight increase from 19,46 to 19,55. This follows the graph in Figure \ref{fig:poly_valid} from the book. 

\myFigure{validationSetApproach_poly.png}{Validation error estimates for a single split}{fig:poly_valid}{0.6}


\section{Leave-one-out cross validation}
Leave-one-out Cross-validation, or \emph{LOOCV}, is closely related to the Validation Set Approach specified in Section \ref{sec:VSA}. The thought process is the same, as you split the data into two parts, a training set and a test set. However, where Validation Set Approach split the data into two subsets of comparable size, the LOOCV uses a single observation as the test set, and the remaining observations are used as the training set. This is illustrated in Figure \ref{fig:loocv}. 

\myFigure{LOOCV.png}{Leave-one-out Cross-validation}{fig:loocv}{0.5}

\subsection{Lab 5.3.2}
For the lab exercise for LOOCV, the \emph{Auto} data is read just like in Lab 5.3.1, but not split into two halves of random values. The \emph{horsepower} and \emph{mpg} data parts are though still distributed into respectively \emph{X\_train} and \emph{Y\_train}.

\begin{lstlisting}[language=Python, label=lst:LOOCV, caption=Leave-one-out Cross-validation loop]
for i in range(1, 6):
	lm = linear_model.LinearRegression()
	poly = PolynomialFeatures(degree=i)
	X_poly_train = poly.fit_transform(X_train)
	
	kf = KFold(n_splits=X_train.shape[0]) 
	test = cross_val_score(lm, X_poly_train, Y_train, cv = kf, 
			scoring = "neg_mean_squared_error")
	
	print(np.mean(-test))
\end{lstlisting}

In Listing \ref{lst:LOOCV} the LOOCV loop is done. Lines 2-4 are the same functionality as in Listing \ref{lst:poly2_valid}, where polynomials are used for the Validation Set Approach. However, here the degrees are iteratively increasing up to a fifth degree polynomial.

\emph{KFold} is used to split the set in as many folds as there are rows, i.e. making 392 folds. This is the "Leave-one-out" part, as there are now as many folds as there are observations. 

It is then all used in the \emph{cross\_val\_score} function. It uses the fitted model, the \emph{horsepower} training set fitted to an i'th degree polynomial, the \emph{mpg} training set and the folds to evaluate a score by Cross Validating.

The Mean Squared Error is lastly printed from the result of the Cross-validation, and is illustrated in Figure \ref{fig:result_LOOCV}.

\myFigure{LOOCV_result.png}{Result of the Leave-one-out Cross-validation}{fig:result_LOOCV}{0.3}

Once again it is clear that there is improvement when going to second degree polynomial, but from there no clear improvement can be seen. 


\section{K-fold Cross-validation}
K-fold Cross-validation is a lot like the LOOCV. However, instead of using only one observation as the validation set, the dataset is split into k roughly equal-sized parts, called \emph{folds}, and one of these folds is the validation set. All other folds are used as training set. It is then possible to iterate through which fold is the validation set to get the best results, but this can be computationally expensive. The idea behind K-fold Cross-validation is illustrated in Figure \ref{fig:K_fold}, where the blue folds are the training sets and the orange fold is the validation set. 

\myFigure{K_fold.png}{Representation of K-fold Cross-validation}{fig:K_fold}{0.5}

\subsection{Lab 5.3.3}

