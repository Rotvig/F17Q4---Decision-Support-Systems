\chapter{Discussion}
\label{chp:disc}
On the basis of the different subjects presented in the previous chapters, this chapter will discuss the different aspects of these  topics.

\subsection{Linear regression}

Firstly linear regression was presented in the report as a simple method for predicting a quantitative response variable. The method assumes a linear relationship between the response and the predictors but can be extended to perform better for non-linear data. This can be done by using polynomial regression which was used in the exercises in chapter \ref{chp:crossvalidation}. In general linear regression though have drawbacks, it assumes the form of the function. For example assuming a linear relationship between the predictors and response variable, when the relations is not linear, will result in a very poor fitting of the model. In cases were more flexibility is needed non-parametric approaches such as k-nearest neighbor can be used. Another limitation of linear regression is that it predicts a quantitative response value. As discussed in chapter \ref{chp:logreg} it is possible to modify linear regression for classification but it performs poorly.

\subsection{Classification}

For classification logistic regression, linear discriminant analysis, quadratic discriminant analysis and K-nearest neighbor classifier was presented. When choosing between logistic regression or linear discriminant analysis an important aspect is that linear discriminant analysis assumes observations to have a Gaussian distribution. If this assumptions is true, then linear discriminant analysis will likely outperform logistic regression and vice versa. So choosing either logistic regression or linear discriminant analysis mainly depends on whether your observations does have a Gaussian distribution or not. It was also noted that well-seperated datasets can be problematic for logistic regression because it might not converge.
 
A more flexible but similar classifier to linear discriminant analysis was also presented, the quadratic discriminant analysis. It instead assumes a separate covariance matrix for each class. This can potentially lead to better prediction if this is the case. This also gives a quadratic decision boundary instead of a linear one. For linear data, linear discriminant analysis is more appropriate because quadratic discriminant analysis is likely to fit a more flexible classifier than necessary.

For even more complex decision boundaries, K-nearest neighbor classifier might outperform QDA. K-nearest neighbor classifier is also a non-parametric classifier which makes it very flexible.

\subsection{Resampling methods}
Different resampling methods were presented in the report. These techniques refit a model to samples from the training set to get information about the model. 

Validation-set approach and cross validation is two approaches to obtain information about the test-set prediction error. In general training and testing on the same data leads to an underestimation of the error rate. Techniques such as validation set approach and cross validation was presented in the report to overcome this problem. Validation set approach was a very simple technique and was not computationally expensive. The validation set approach splits the data in two equally sized parts. The model is then fit on only half the data which often leads to an overestimation of the test error, compared to when training is done on all the data. If a lot of data is available this problem might not be as big. Another method is to use cross-validation which allows to iterate through splits of training and testing data. This makes for a smart way of utilizing the available data, but it is also computationally expensive.

The boostrap can also be used for estimating properties of a prediction model, this could for example be the standard errors of the coefficients which was done in the exercises. Bootstrapping does this by simulating from data sets from the true data set. Even though bootstrap showed a simple tool in the exercise presented in the report, it can become cumberstone with more complex data sets.

\subsection{Subset selection}
A difficult task when fitting a model is choosing the best and the right amount of predictors. Best subset selection, forward and backward stepwise selection was presented as techniques for this. Best subset selection tries all combinations to find the most optimal, but this has the drawback of being very computationally expensive and not feasible if the number of predictors is too high. Forward and backward stepwise selection is a computationally less expensive estimation of the best predictors. When choosing the amount of predictors for a model it is also important to be aware if a smaller number of predictors performs approximately equally as good as one with more predictors, if this is the case then simplicity of the model might be preferred

\subsection{Unsupervised learning}
The previous predictions methods mentioned are all supervised methods. Another case described in this report is were the true label is unknown. Two different methods were presented in this report for unsupervised learning, namely k-means clustering and hierarchical clustering. K-means clustering requires a prespecified  number of clusters which might be difficult to know. If this is not possible one could consider using hierarchical clustering instead. Note though that hierarchical clustering assumes that the data has a hierarchical structure, if this is not true, then k-means is likely to perform better. In general both algorithms will cluster all observations and even outliers are forced into a cluster, which might result in distorted clusters.