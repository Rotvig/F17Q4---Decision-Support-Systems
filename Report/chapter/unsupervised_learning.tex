\chapter{Unsupervised Learning}
\label{chp:unsuplea}
In unsupervised learning there is no response variable Y. There is only a set of features $X_1, X_2,..., X_p$ measured on n observations. So the goal is to find the interesting parts of the measurements on the features.
Supervised learning compared to unsupervised learning comes with some challenges. The tasks tends to be more subjective and there is no simple goal such as prediction of a response as there is in supervised learning. Furthermore there is no accepted method for performing cross-validation, due to the fact that there is no Y to check results up against.
But it is an important tool to predict subgroups when the number of subgroups are unknown.
In this section the focus will be on the  technique called \emph{clustering} which is used to discover subgroups or clusters in a data set. 
This is done by dividing the data set into clusters so that the observations in each of the clusters are similar to each other, and the observations belonging to other clusters are different compared to the observations in other clusters.
The problem is to determine if observations are similar or different to each other.
 
\section{K-Means Clustering}
\label{chp:clus}
K-means clustering is a technique to divide some observations in a data set into a prespecified \emph{k} number of clusters. When \emph{k} is specified it will give each of the observations to one of the clusters within 1 - \emph{k}. To understand this, it is easier to look at an example. In figure \ref{fig:K-meanex} a data set is assigned to three clusters. This means that \emph{k} is equal to 3. The data set consists of 50 observations. Each cluster is marked with a color and the cross is the center points of a cluster. This point is also called a \emph{centroid}.

\myFigure{K-mean.PNG}{Example of the K-means algorithm where k is 3}{fig:K-meanex}{0.55}

K-means clustering needs to satisfy two properties. 
\begin{itemize}
	\item Each observation belongs to one of the clusters
	\item Clusters cannot overlap, which means that no observation can belong to more than one cluster
\end{itemize} 

To get a good clustering the \emph{within-cluster variation} needs to be as small as possible. So the algorithm will divide the observation into k clusters, such that the total sum of all \emph{within-cluster variation} is as small as possible. The \emph{within-cluster variation} is commonly calculated with the \emph{squared Euclidean distance}. To obtain the correct result all Euclidean distances are divided by the number of observations in the given cluster. Now it will try to minimize the total \emph{within-cluster variation} summed over all clusters.
The K-means clustering algorithm can then be written as in algorithm \ref{algo:KMeansClustering}.

\begin{algorithm}
	\caption{K-Means Clustering}
	\label{algo:KMeansClustering}
	\begin{algorithmic}[1]
		\State A random number from 1 to k is chosen for each observation. This number will act as an initial cluster assignment for every given observation.
		
		\State 
		Iterate until the observations stop changing between clusters.
		\begin{enumerate}[label=(\alph*)]
			\item The centroids are calculated for each of the k clusters and these centroids acts as the vector for the means for each observation in the k'th cluster.
			
			\item For each observation the Euclidean distance to each centroid is calculated and the observation is assigned to the closest one.
		\end{enumerate}
	\end{algorithmic}
\end{algorithm}

\subsection{Lab 10.5.1 - K-Means Clustering}
First a random data set consisting of 50 observations is generated. After this, the K-means from the sklearn library is used to perform the K-means clustering.
After the algorithm is used, the labels and the centroids are saved. The code is shown in listing \ref{lst:kmeanscode} below with number of clusters set to k = 2.
\begin{lstlisting}[language=Python, label=lst:kmeanscode, caption=The KMeans function]
k = 2
kmeans = KMeans(n_clusters=k, init="k-means++",n_init= 20).fit(zip(X,Y))
Labels = kmeans.labels_
centroids = kmeans.cluster_centers_
\end{lstlisting}

The K-means also takes two other parameters. Namely the \emph{init} and the \emph{n\_init}. Init states which approach is used, and in this case the \emph{K-means++} approach is used. The n\_init is how many times the approach should be performed. Here it is set to 20.

Now the task is to plot the labels and the centroids so the two clusters can be visualized. 
This is done with the code shown in listing \ref{lst:kmeansplot}.

\begin{lstlisting}[language=Python, label=lst:kmeansplot, caption=The code for plotting the two clusters]
lines = plt.plot(centroids[:,0], centroids[:,1], 'kx')
plt.setp(lines, ms=15.0)
plt.setp(lines, mew=3.0)

for i in range(k):
	dsx = X[np.where(Labels==i)]
	dsy = Y[np.where(Labels==i)]
	plt.plot(dsx, dsy, 'o')

plt.show()
\end{lstlisting}

Initially the two centroids are plotted. This is done from line 1 to 3. The \emph{kx} is written so they are labeled with a cross. For plotting the observations a for loop is used. It runs through the two possible clusters. The \emph{o} is added so the observations are plotted as a dot. 
The result of this code can be seen in figure \ref{fig:kmeansResult}.

\myFigure{KmeansResult.PNG}{The plotted result of lab 10.5.1}{fig:kmeansResult}{0.6}

The centroids are marked with crosses and each observation is either blue or green depending on which cluster it is assigned too.


\section{Hierarchical Clustering}
Hierarchical clustering does not require a prespecified \emph{k} value as K-means clustering has to.
Furthermore hierarchical clustering produces a \emph{dendrogram} which shows the observations as a tree structure.
The most common type is called the \emph{agglomerative} clustering, it is also known as the \emph{bottom-up} approach.

\subsection{Dendrograms}
The easiest way of explaining a dendrogram is to show one. So figure \ref{fig:dendrogram} shows a dengrogram which is constructed of a data set with 50 observations.
The labels are shown in different colors.

One of the leafs are equal to one of the 50 observations. 
Depending on the data set and the observations the dendrogram can help to determine the number of clusters that fits the data best. In this example, the number of clusters is two. Line number one shows where to cut in the dendrogram to get the number of clusters to be two. Line number two shows a different cut, and if this was best for the data set the number of clusters would be three.
Observations that fuse close to the top of the dendrogram is often more different from each other compared to the observations in the bottom of the dendrogram. 

\myFigure{Dendrogram.PNG}{Dendrogram with 50 observations}{fig:dendrogram}{1}

\FloatBarrier
To show how this works, a simple data set containing 7 observations is shown in figure \ref{fig:dendrogramdata} to the right.
It can be seen that the observations 5 and 7 are close to each other, and observations 1 and 6 are also close to each other. These are the bottom of the dendrogram, as it can been seen in the left side of figure \ref{fig:dendrogramdata}. Now the dendrogram is expanded, and by looking at the next observation that are closest to 5 and 7. Clearly 3 is the closest one, so this is the next leaf in the dendrogram. The same is done for observations 1 and 6 and at the end the dendrogram will look like figure \ref{fig:dendrogramdata} to the left.

\myFigure{DendrogramData.PNG}{Illustration of a dendrogram and its data set}{fig:dendrogramdata}{1}

\FloatBarrier
\subsection{The Hierarchical Clustering Algorithm}
To produce the dendrogram the hierarchical clustering algorithm is used. First step is to find the Euclidean distance between the pair of observations and find the dissimilarities.
Then it runs iteratively trough all the observations. It starts at the bottom and finds the two observations acting as their own cluster, that are closest and fuses those two, so that there now are $n-1$ clusters. This continues, so the next two clusters that are most similar are fused, making the number of clusters $n-2$. This is done until all observations belong to one big cluster.

The hierarchical clustering algorithm is shown in algorithm \ref{algo:HierarchicalClustering}.

\begin{algorithm}
	\caption{Hierarchical Clustering}
	\label{algo:HierarchicalClustering}
	\begin{algorithmic}[1]
 		\State Begin with the number n of observations and the measure of all the $(\dfrac{n}{2}) =n(n-1)/2 $ pairwise dissimilarities. Each of the observations will treat this as its cluster.
 		
 		\State for \emph{i} $= n,n-1,...,2:$
 		\begin{enumerate}[label=(\alph*)]
 			\item All pairwise inter-cluster dissimilarities matching the $i$ cluster are examined and then the pairs that are most similar are identified and fused into one cluster. This indicates at which height to place the fusion in the dendrogram.
 			\item Compute the new pairwise inter-cluster dissimilarity for the remaining $\emph{i}-1$ clusters.
 		\end{enumerate}
 	\end{algorithmic}
 \end{algorithm}

But one issue has not been addressed. The algorithm needs to be capable of defining the dissimilarities between two clusters if one or both contains multiple observations. This is done with an extension called \emph{linkage} which defines dissimilarities between two groups of observations. The most common types of linkage are \emph{complete}, \emph{average}, \emph{single} and
\emph{centroid}. These are explained in table \ref{table:linkage}

\begin{table}
	\begin{center}
		\begin{tabular}{ | l | p{12cm} |}
			\hline
			Linkage & Description \\ \hline
			Complete & Maximal intercluster dissimilarity. Record the largest of these dissimilarities in cluster A and B \\ \hline
			Single & Minimal intercluster dissimilarity. Record the smallest of these dissimilarities in cluster A and B \\ \hline
			Average & Mean intercluster dissimilarity. Record the average of pairwise dissimilarities in cluster A and B\\
			\hline
			Centroid & Dissimiliarity between the centroids for cluster A and B and record the average of these dissimilarities
			\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Different linkage methods}
	\label{table:linkage}
\end{table}


Complete and average linkage are generally the most preferred linkage as they produce more balanced dendrograms. Sometimes you need to scale your data set to obtain standard deviation. Then new problems arises, because of which dissimilarity measure should be used and which linkage should be used. This is a difficult problem and there is no standard answer.

\subsection{Lab 10.5.2 - Hierarchical Clustering}
A data set containing 50 observations is randomly generated and assigned in X and Y coordinates. So it is the same data set structure from lab 10.5.1.
The hierarchical clustering dendrogram will be produced using complete, single and average linkage with Euclidean distance.
To perform the hierarchical clustering the library scipy is imported. This provides the functions for creating dendrograms with the linkage needed. 
The code for creating the three types of linkage are shown in listing \ref{lst:Linkage}.

\begin{lstlisting}[language=Python, label=lst:Linkage, caption=The code that applies the three different linkage on the data set]
complete = linkage(X, method='complete', metric='euclidean')
single = linkage(X, method='single', metric='euclidean') 
average = linkage(X, method='average', metric='euclidean')
\end{lstlisting}

As it can be seen, the linkage functions look a lot like each other. The only difference is the method. The two other parameters are the data set and the one that specifies which metric it should use. In this lab the linkage uses Euclidean distance. 
To create the dendrograms the function \emph{dendrogram} from the scipy library is used. The only parameter that is needed to build the dendrograms is the three linkage objects that was made in listing \ref{lst:Linkage}.

When plotting the dendrograms the only difference is which linkage object is parsed as a parameter.
The result of each of the three different dendrograms can be seen in figures \ref{fig:dendrogramsCompAvg} and \ref{fig:dendrogramsingle}.


\mySubFigure{DendrogramComplete.PNG}{DendrogramAverage.PNG}{Dendrograms for linkage methods complete and Average}{Complete}{Average}{fig:dendrogramsCompAvg}{fig:denComplete}{fig:denAverage}


\myFigure{DendrogramSingle.PNG}{Dendrogram for the linkage method single}{fig:dendrogramsingle}{0.7}

As expected the dendrograms are different from each other.
Looking at figures \ref{fig:denComplete} and \ref{fig:denAverage} average and complete looks more balanced compared to figure \ref{fig:dendrogramsingle} where the method single is used. It can be seen that by using single linkage one observation belongs to its own cluster, and two other observations belong to their own cluster.
This could indicate that two clusters are not the best solution, and it could be more sensible to divide the data set into more clusters. In this case four clusters would be better. When performed in the exercise this proved to be correct and improved the clustering. 