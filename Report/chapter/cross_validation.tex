\chapter{Cross-validation}
\label{chp:crossvalidation}
In this chapter the validation set approach is presented as a simple way of estimating the test error for a model. Furthermore Cross-validation  is explained. Cross-validation is one of two resampling methods in Statistical Learning, the other being the Bootstrap which will be covered in the next chapter. Cross-validation is done by splitting up the data set in different ways, so that training and testing is done on varying samples. 

\section{The validation set approach}
\label{sec:VSA}
In the Validation Set Approach the available set of samples is randomly divided into two parts of approximately equal size. One is the training set and the other is the validation set. The idea is then to fit the model on the training set, and then apply the model to the validation set. The resulting error of the validation set is then used as an estimate of the test error. This error is typically found as the Mean Squared Error. 

\myFigure{validationSetApproach.png}{Splitting data randomly}{fig:valid}{0.6} 

The concept of randomly splitting the dataset is illustrated in Figure \ref{fig:valid}. The dataset is a series of observations from 1 to n, and the result is two almost equally sized parts, with the observations from the original dataset randomly distributed. 

The blue half of the dataset is the training set, this is used for fitting the model. The orange half of the data set is the validation set, used for estimating the prediction error rate.


\subsection{Lab 5.3.1 The validation set approach}
In this exercise the goal is to use the validation set approach. So as described in the section above the first thing to do is to split the data randomly in two halves. The data used here is the \emph{Auto} dataset, containing a list of car information. 

\begin{lstlisting}[language=Python, label=lst:fit_valid, caption=Fit linear regression]
train, test = train_test_split(df, test_size = 0.5)

X_train = train[['horsepower']]
X_test = test[['horsepower']]

Y_train = train['mpg']
Y_test = test['mpg']

lm = linear_model.LinearRegression()
lm.fit(X_train, Y_train)
\end{lstlisting}

Listing \ref{lst:fit_valid} shows the \emph{Auto} data as \emph{df} being split randomly in half by the \emph{train\_test\_split()} function. 
\emph{horsepower} is used as a predictor and denoted X while \emph{mpg} is the response value denoted Y. The model is then fitted on the training data \emph{X\_train} and \emph{Y\_train}.

\begin{lstlisting}[language=Python, label=lst:print_valid, caption=Print Mean Squeared Error]
print("Mean squared error: %.2f" 
		% np.mean((lm.predict(X_test) - Y_test) ** 2))
\end{lstlisting}

In Listing \ref{lst:print_valid} the Mean Squared Error is computed by applying the fitted model to the test sets. The error is 24,42. 

Because the relationship between \emph{horsepower} and \emph{mpg} is non-linear the model can be improved upon by using higher polynomial features for fitting the model. Using second degree polynomials is shown in Listing \ref{lst:poly2_valid}. Third degree polynomials was also applied in the exercise.

\begin{lstlisting}[language=Python, label=lst:poly2_valid, caption=Polynomial features with degree = 2]
poly = PolynomialFeatures(degree=2)
X_poly_train_deg2 = poly.fit_transform(X_train)
X_poly_test_deg2 = poly.fit_transform(X_test)

lm.fit(X_poly_train_deg2, Y_train)
\end{lstlisting}

Using polynomials shows a clear improvement of the Mean Squared Error, but not continuously. Using the second degree polynomial shows a drop in the Mean Squared Error from 24,42 to 19,46, but using the third degree polynomial shows a slight increase from 19,46 to 19,55. 

\section{Leave-one-out cross validation}
In validation-set approach only half of the data is used for training the model. This might give an overestimate of the mean squared error when training on the complete data set. The MSE might also vary a lot when doing random splits of the dataset. Leave-one-out Cross-validation tries to address this drawback and is illustrated in figure \ref{fig:loocv}. It splits the dataset in number of observations n and then uses n-1 observations for training and the last one as validation set. It then iterates through using all observations as validation. This might address the drawbacks of validation set approach but it is also very time consuming as a model has to be fitted n times.

\myFigure{LOOCV.png}{Leave-one-out Cross-validation}{fig:loocv}{0.52}

\subsection{Lab 5.3.2 Leave-one-out cross-validation}
For the exercise for LOOCV, the \emph{Auto} data is read just like in Lab 5.3.1, but not split into two halves of random values. The \emph{horsepower} and \emph{mpg} data parts are though still distributed into respectively \emph{X\_train} and \emph{Y\_train}. 

\begin{lstlisting}[language=Python, label=lst:LOOCV, caption=Leave-one-out Cross-validation loop]
for i in range(1, 6):
	lm = linear_model.LinearRegression()
	poly = PolynomialFeatures(degree=i)
	X_poly_train = poly.fit_transform(X_train)
	
	kf = KFold(n_splits=X_train.shape[0]) 
	test = cross_val_score(lm, X_poly_train, Y_train, cv = kf, 
			scoring = "neg_mean_squared_error")
	
	print(np.mean(-test))
\end{lstlisting}

In Listing \ref{lst:LOOCV} the LOOCV is done. Lines 2-4 are the same functionality as in Listing \ref{lst:poly2_valid}, where polynomials are used for the Validation Set Approach. However, here the degrees are iteratively increasing up to a fifth degree polynomial.

\emph{KFold} is used for leave-one-out validation to split the set in as many folds as there are observations, i.e. making 392 folds.

It is then all used in the \emph{cross\_val\_score} function. It uses the fitted model, the \emph{horsepower} training set fitted to an i'th degree polynomial, the \emph{mpg} training set and the folds to evaluate a score by Cross Validation.

%The Mean Squared Error is lastly printed from the result of the Cross-validation, and is illustrated in Figure \ref{fig:result_LOOCV}.

%\myFigure{LOOCV_result.png}{Result of the Leave-one-out Cross-validation}{fig:result_LOOCV}{0.3}

The Mean Squared Error is lastly computed from the result of the Cross-validation. Once again it is clear that there is improvement when using second degree polynomial, but from there no clear improvement can be seen. Here the mse is a bit lower than with the validation set approach. This can be caused by the discussed overestimation of the mse by only training on half the dataset. For first degree polynomial the mse is 24.23, for second degree polynomial 19.25 and for third degree polynomial the mse was calculated to 19.33. All of these error rates showed lower than with validation set approach


\section{K-fold Cross-validation}
K-fold Cross-validation is similar to LOOCV. However, instead of using only one observation as the validation set, the dataset is split into k roughly equal-sized parts, called \emph{folds}, and one of these folds is the validation set. All other folds are used as training set. Similar to LOOCV the algorithm then iterates through which fold is the validation set. The idea behind k-fold cross-validation is illustrated in Figure \ref{fig:K_fold}, where the blue folds are the training sets and the orange fold is the validation set. Note that the case where k is equal the number of observations is what was just described as LOOCV. Having a lower value of k therefore provides a less computationally expensive way of doing cross validation.

\myFigure{K_fold.png}{Representation of K-fold Cross-validation}{fig:K_fold}{0.5}

\FloatBarrier
\subsection{Lab 5.3.3}
The only difference from Lab 5.3.2 to this lab exercise is that now the number of folds is set by k, and not the amount of observations. In Listing \ref{lst:K_fold} it can be seen that the number of splits is set to 10, as the exercise specifies $k=10$, and the loop now runs 10 times up until a 10th degree polynomial. The rest is similar to Listing \ref{lst:LOOCV}.

\begin{lstlisting}[language=Python, label=lst:K_fold, caption=K-fold Cross-validation loop]
for i in range(1, 11):
	...
	k_fold = KFold(n_splits=10, shuffle=True) 
	...
\end{lstlisting}
\FloatBarrier
In table \ref{table:kfold_polynomials_mse} the result of the K-fold Cross-validation is shown from a 1 to 10 degree polynomial. The mean squared error is lower than with validation set approach. Compared to LOOCV it has a higher MSE for some polynomials while for others the MSE is lower. As explained this method is computationally less expensive than LOOCV and this was also obvious when executing the implementation.

\begin{table}	
	\centering
	\begin{subtable}[t]{2in}
		\centering
		\begin{tabular}{ p{2.5cm} p{1.5cm}  }
			\textbf{Degree} & \textbf{MSE} \\
			\hline 
			\\
			1 & 24.22 \\\hline
			\\
			2 & 19.19 \\\hline
			\\
			3 & 19.28 \\\hline
			\\
			4 & 19.54  \\\hline
			\\
			5 & 19.00  \\\hline
		\end{tabular}
		\label{table:mse_validation}
	\end{subtable}
	\quad 
	\begin{subtable}[t]{2in}
		\centering
		\begin{tabular}{ p{2.5cm} p{1.5cm}  }
			\textbf{Degree} & \textbf{MSE} \\
			\hline 
			\\
			6 & 18.85 \\\hline
			\\
			7 & 19.48  \\\hline
			\\
			8 & 19.41  \\\hline
			\\
			9 & 19.02  \\\hline
			\\
			10 & 19.04  \\\hline
		\end{tabular}
		\label{table:mse_cross}
	\end{subtable}
	\caption{Polynomials from 1 to 10 and calculated mean squared error.}\label{table:kfold_polynomials_mse}
\end{table}
