\chapter{Conclusion}
\label{chp:conc}

Throughout this report the theory of different subjects within statistical learning was elaborated upon. Related to each topic exercises were implemented in Python to give a deeper understanding for each method. 

Different methods for prediction and how they differ was also discussed. Linear regression was the only method presented for predicting a quantitative response variable but it showed less appropriate for classification. For classification logistic regression and linear discriminant analysis were explained. Choosing between these too mainly depends on if the classes had a Gaussian distribution. If this was the case then linear discriminant analysis would likely outperform logistic regression and vice versa. For more non-linear data with more complex decision boundaries quadratic discriminant analysis was presented. If the decision boundary was even more complex then K-nearest neighbor classifier was presented as an option. Therefore no method can be seen as better than other, choosing between the prediction methods is all about the type of response and the data.

Resampling methods for getting information about the properties of the models was also explained in the report. Bootstrap can be used for quantifying uncertainty in a model. It showed a good fit for estimating the standard error of coefficients in the exercises. For more complex data the bootstrap method might not be applied as simply.
Validation set approach and cross validation were presented as methods for estimating the test error. Validation set approach was a more simple and less computationally expensive approach but it can lead to and overestimation of the test error. Leave-one-out cross validation and k-fold cross validation was also presented in general a 5 or 10 fold cross validation is preferred as a good comprise between the bias-variance trade-off.

For subset selection best subset selection and forward and backward stepwise selection was explained. Best subset selection estimates the most optimal numbers of predictors and which predictors to use more precisely because it iterates through all combinations. However, when more than 40 predictors are available, then best subset selection becomes infeasible. Instead forward and backward stepwise selection can be used as two lot more efficient methods which both often estimates similar models to best subset selection.

When the true label is unknown then unsupervised learning techniques can be used instead. Unlike supervised learning there is no simple goal and the analysis is often more exploratory. K-means clustering and hierarchical clustering was presented in the report. Hierarchical needs no previous specified number of clusters but assumes hierarchical structure of the data. If the data is in fact has a hierarchical then this is likely to estimate the clusters well. If the data does not have a hierarchical structure then k-means is preferred even though the numbers of clusters must be prespecified. 

The report has given an introduction to all the topics presented in the course \emph{Decision support systems}. Furthermore it has given insight in the advantages and disadvantages of the different methods and understanding when a method is applicable.




